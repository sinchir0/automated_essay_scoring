{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_NAME = \"e-word-tfidf\"\n",
    "MODEL_NAME = \"tfidf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"data\"\n",
    "SEED = 42\n",
    "N_FOLD = 3\n",
    "\n",
    "# path setting\n",
    "COMPETITION_NAME = \"automated_essay_scoring\"\n",
    "DATASET_NAME = f\"{EXP_NAME}-{MODEL_NAME.split('/')[-1]}\"\n",
    "MODEL_OUTPUT_PATH = f\"trained_models/{EXP_NAME}\"\n",
    "\n",
    "UPLOAD_DATA_TO_KAGGLE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_dataset_name(dataset_name: str) -> None:\n",
    "    if len(dataset_name) < 6 or len(dataset_name) > 50:\n",
    "        raise Exception(\n",
    "            f\"データセットの文字列は6~50文字にしてください。現在{len(DATASET_NAME)}文字\"\n",
    "        )\n",
    "    if \"_\" in dataset_name:\n",
    "        raise Exception(\"datasetの名称に_の使用は禁止です\")\n",
    "\n",
    "\n",
    "validate_dataset_name(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/shinichiro.saito/automated_essay_scoring/automated_essay_scoring/eda\n",
      "Local Mac!\n",
      "../../data\n",
      "/Users/shinichiro.saito/automated_essay_scoring/automated_essay_scoring/eda\n",
      "Local Mac!\n",
      "../../trained_models/e-word-tfidf\n"
     ]
    }
   ],
   "source": [
    "def resolve_path(base_path: str) -> str:\n",
    "    import os\n",
    "\n",
    "    cwd = os.getcwd()\n",
    "    print(cwd)\n",
    "    if cwd == f\"/notebooks\":\n",
    "        print(\"Jupyter Kernel By VSCode!\")\n",
    "        return f\"/notebooks/{COMPETITION_NAME}/{base_path}\"\n",
    "    elif cwd == f\"/notebooks/{COMPETITION_NAME}\":\n",
    "        print(\"nohup!\")\n",
    "        return base_path\n",
    "    elif cwd == f\"/notebooks/{COMPETITION_NAME}/{COMPETITION_NAME}/exp\":\n",
    "        print(\"Jupyter Lab!\")\n",
    "        return f\"../../{base_path}\"\n",
    "    elif cwd.startswith(\"/Users\"):\n",
    "        print(\"Local Mac!\")\n",
    "        return f\"../../{base_path}\"\n",
    "    else:\n",
    "        raise Exception(\"Unknown environment\")\n",
    "\n",
    "\n",
    "DATA_PATH = resolve_path(DATA_PATH)\n",
    "print(DATA_PATH)\n",
    "MODEL_OUTPUT_PATH = resolve_path(MODEL_OUTPUT_PATH)\n",
    "print(MODEL_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(MODEL_OUTPUT_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed the same seed to all\n",
    "def seed_everything(seed: int) -> None:\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pl.read_csv(f\"{DATA_PATH}/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{DATA_PATH}/essay_id_fold_by_s_sl_g_p_only_train_dict.json\") as f:\n",
    "    essay_id_fold_only_train = json.load(f)\n",
    "\n",
    "train = train.with_columns(\n",
    "    pl.col(\"essay_id\")\n",
    "    .replace(essay_id_fold_only_train, return_dtype=pl.Int64)\n",
    "    .alias(\"fold\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>essay_id</th><th>full_text</th><th>score</th><th>fold</th></tr><tr><td>str</td><td>str</td><td>i64</td><td>i64</td></tr></thead><tbody><tr><td>&quot;000d118&quot;</td><td>&quot;Many people have car where the…</td><td>3</td><td>0</td></tr><tr><td>&quot;000fe60&quot;</td><td>&quot;I am a scientist at NASA that …</td><td>3</td><td>0</td></tr><tr><td>&quot;001ab80&quot;</td><td>&quot;People always wish they had th…</td><td>4</td><td>1</td></tr><tr><td>&quot;001bdc0&quot;</td><td>&quot;We all heard about Venus, the …</td><td>4</td><td>0</td></tr><tr><td>&quot;002ba53&quot;</td><td>&quot;Dear, State Senator\n",
       "\n",
       "This is a…</td><td>3</td><td>2</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 4)\n",
       "┌──────────┬─────────────────────────────────┬───────┬──────┐\n",
       "│ essay_id ┆ full_text                       ┆ score ┆ fold │\n",
       "│ ---      ┆ ---                             ┆ ---   ┆ ---  │\n",
       "│ str      ┆ str                             ┆ i64   ┆ i64  │\n",
       "╞══════════╪═════════════════════════════════╪═══════╪══════╡\n",
       "│ 000d118  ┆ Many people have car where the… ┆ 3     ┆ 0    │\n",
       "│ 000fe60  ┆ I am a scientist at NASA that … ┆ 3     ┆ 0    │\n",
       "│ 001ab80  ┆ People always wish they had th… ┆ 4     ┆ 1    │\n",
       "│ 001bdc0  ┆ We all heard about Venus, the … ┆ 4     ┆ 0    │\n",
       "│ 002ba53  ┆ Dear, State Senator             ┆ 3     ┆ 2    │\n",
       "│          ┆                                 ┆       ┆      │\n",
       "│          ┆ This is a…                      ┆       ┆      │\n",
       "└──────────┴─────────────────────────────────┴───────┴──────┘"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=lambda x: x,\n",
    "    preprocessor=lambda x: x,\n",
    "    token_pattern=None,\n",
    "    strip_accents=\"unicode\",\n",
    "    analyzer=\"word\",\n",
    "    ngram_range=(3, 6),\n",
    "    min_df=0.05,\n",
    "    max_df=0.95,\n",
    "    sublinear_tf=True,\n",
    ")\n",
    "vectorizer.fit(train[\"full_text\"])\n",
    "all_voc = vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19627"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shinichiro.saito/automated_essay_scoring/.venv/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:1381: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shinichiro.saito/automated_essay_scoring/.venv/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:1381: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shinichiro.saito/automated_essay_scoring/.venv/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:1381: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import pickle\n",
    "\n",
    "\n",
    "oofs: list[pd.DataFrame] = []\n",
    "# Cross Validationによる学習の実施\n",
    "for fold in range(N_FOLD):\n",
    "    print(f\"Start fold {fold}\")\n",
    "\n",
    "    # foldごとにtrainとvalidに分ける\n",
    "    train_fold = train.filter(pl.col(\"fold\") != fold)\n",
    "    valid_fold = train.filter(pl.col(\"fold\") == fold)\n",
    "\n",
    "    def tfidf_tokenizer(x):\n",
    "        return x\n",
    "\n",
    "    def tfidf_preprocessor(x):\n",
    "        return x\n",
    "\n",
    "    # TfidfVectorizer parameter\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        tokenizer=tfidf_tokenizer,\n",
    "        preprocessor=tfidf_preprocessor,\n",
    "        token_pattern=None,\n",
    "        strip_accents=\"unicode\",\n",
    "        analyzer=\"word\",\n",
    "        ngram_range=(3, 6),\n",
    "        min_df=0.05,\n",
    "        max_df=0.95,\n",
    "        sublinear_tf=True,\n",
    "        vocabulary=all_voc,\n",
    "    )\n",
    "\n",
    "    vectorizer.fit(train_fold[\"full_text\"])\n",
    "    valid_tfid = vectorizer.transform(valid_fold[\"full_text\"])\n",
    "\n",
    "    dense_matrix = valid_tfid.toarray()\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        dense_matrix,\n",
    "        columns=[f\"tfidf_{i}\" for i in range(len(all_voc))],\n",
    "    )\n",
    "\n",
    "    df[\"essay_id\"] = valid_fold[\"essay_id\"]\n",
    "\n",
    "    oofs.append(df)\n",
    "\n",
    "    # save vectorizer\n",
    "    with open(\n",
    "        f\"{MODEL_OUTPUT_PATH}/tfidf_vectorizer_s_sl_g_p_fold{fold}.pkl\", \"wb\"\n",
    "    ) as file:\n",
    "        pickle.dump(vectorizer, file)\n",
    "\n",
    "all_tfidf_res = pd.concat(oofs)\n",
    "\n",
    "# SVD : 次元削減\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=100)\n",
    "all_tfidf_res_reduced = svd.fit_transform(\n",
    "    all_tfidf_res[[f\"tfidf_{i}\" for i in range(len(all_voc))]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_tfidf = pd.DataFrame(\n",
    "    all_tfidf_res_reduced,\n",
    "    columns=[f\"tfidf_{i}\" for i in range(100)],\n",
    ")\n",
    "oof_tfidf[\"essay_id\"] = all_tfidf_res[\"essay_id\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf_0</th>\n",
       "      <th>tfidf_1</th>\n",
       "      <th>tfidf_2</th>\n",
       "      <th>tfidf_3</th>\n",
       "      <th>tfidf_4</th>\n",
       "      <th>tfidf_5</th>\n",
       "      <th>tfidf_6</th>\n",
       "      <th>tfidf_7</th>\n",
       "      <th>tfidf_8</th>\n",
       "      <th>tfidf_9</th>\n",
       "      <th>...</th>\n",
       "      <th>tfidf_91</th>\n",
       "      <th>tfidf_92</th>\n",
       "      <th>tfidf_93</th>\n",
       "      <th>tfidf_94</th>\n",
       "      <th>tfidf_95</th>\n",
       "      <th>tfidf_96</th>\n",
       "      <th>tfidf_97</th>\n",
       "      <th>tfidf_98</th>\n",
       "      <th>tfidf_99</th>\n",
       "      <th>essay_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.428340</td>\n",
       "      <td>-0.039073</td>\n",
       "      <td>-0.040121</td>\n",
       "      <td>-0.047820</td>\n",
       "      <td>0.024961</td>\n",
       "      <td>0.053059</td>\n",
       "      <td>0.302992</td>\n",
       "      <td>-0.056375</td>\n",
       "      <td>0.063701</td>\n",
       "      <td>0.016800</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010025</td>\n",
       "      <td>0.014342</td>\n",
       "      <td>-0.002540</td>\n",
       "      <td>0.014464</td>\n",
       "      <td>0.012424</td>\n",
       "      <td>-0.014837</td>\n",
       "      <td>0.003919</td>\n",
       "      <td>-0.041749</td>\n",
       "      <td>0.040429</td>\n",
       "      <td>000d118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.356387</td>\n",
       "      <td>0.159323</td>\n",
       "      <td>0.009374</td>\n",
       "      <td>0.007485</td>\n",
       "      <td>0.395414</td>\n",
       "      <td>-0.125089</td>\n",
       "      <td>-0.049730</td>\n",
       "      <td>-0.042228</td>\n",
       "      <td>0.030181</td>\n",
       "      <td>-0.008561</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013700</td>\n",
       "      <td>0.033830</td>\n",
       "      <td>-0.000301</td>\n",
       "      <td>0.038543</td>\n",
       "      <td>-0.017799</td>\n",
       "      <td>-0.002648</td>\n",
       "      <td>0.017269</td>\n",
       "      <td>-0.018821</td>\n",
       "      <td>0.018258</td>\n",
       "      <td>000fe60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.421714</td>\n",
       "      <td>0.556962</td>\n",
       "      <td>0.114038</td>\n",
       "      <td>-0.019595</td>\n",
       "      <td>-0.195060</td>\n",
       "      <td>0.011409</td>\n",
       "      <td>-0.023142</td>\n",
       "      <td>0.032614</td>\n",
       "      <td>-0.004093</td>\n",
       "      <td>0.147141</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015431</td>\n",
       "      <td>0.042424</td>\n",
       "      <td>-0.017514</td>\n",
       "      <td>0.008480</td>\n",
       "      <td>-0.000546</td>\n",
       "      <td>0.011427</td>\n",
       "      <td>-0.038294</td>\n",
       "      <td>-0.023080</td>\n",
       "      <td>-0.012667</td>\n",
       "      <td>001bdc0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.377300</td>\n",
       "      <td>0.518570</td>\n",
       "      <td>0.120942</td>\n",
       "      <td>-0.020053</td>\n",
       "      <td>-0.211645</td>\n",
       "      <td>0.022247</td>\n",
       "      <td>-0.047556</td>\n",
       "      <td>0.005338</td>\n",
       "      <td>-0.016251</td>\n",
       "      <td>-0.146453</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004665</td>\n",
       "      <td>0.009040</td>\n",
       "      <td>-0.021966</td>\n",
       "      <td>-0.035676</td>\n",
       "      <td>0.003183</td>\n",
       "      <td>0.004906</td>\n",
       "      <td>0.016478</td>\n",
       "      <td>-0.034712</td>\n",
       "      <td>0.015269</td>\n",
       "      <td>0036253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.287526</td>\n",
       "      <td>-0.045324</td>\n",
       "      <td>-0.053092</td>\n",
       "      <td>-0.061103</td>\n",
       "      <td>0.024412</td>\n",
       "      <td>0.037702</td>\n",
       "      <td>0.495741</td>\n",
       "      <td>0.069833</td>\n",
       "      <td>-0.046911</td>\n",
       "      <td>-0.018658</td>\n",
       "      <td>...</td>\n",
       "      <td>0.064636</td>\n",
       "      <td>0.068888</td>\n",
       "      <td>-0.018376</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.022137</td>\n",
       "      <td>-0.010695</td>\n",
       "      <td>-0.035521</td>\n",
       "      <td>-0.007320</td>\n",
       "      <td>0.035616</td>\n",
       "      <td>0047cb3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    tfidf_0   tfidf_1   tfidf_2   tfidf_3   tfidf_4   tfidf_5   tfidf_6  \\\n",
       "0  0.428340 -0.039073 -0.040121 -0.047820  0.024961  0.053059  0.302992   \n",
       "1  0.356387  0.159323  0.009374  0.007485  0.395414 -0.125089 -0.049730   \n",
       "2  0.421714  0.556962  0.114038 -0.019595 -0.195060  0.011409 -0.023142   \n",
       "3  0.377300  0.518570  0.120942 -0.020053 -0.211645  0.022247 -0.047556   \n",
       "4  0.287526 -0.045324 -0.053092 -0.061103  0.024412  0.037702  0.495741   \n",
       "\n",
       "    tfidf_7   tfidf_8   tfidf_9  ...  tfidf_91  tfidf_92  tfidf_93  tfidf_94  \\\n",
       "0 -0.056375  0.063701  0.016800  ... -0.010025  0.014342 -0.002540  0.014464   \n",
       "1 -0.042228  0.030181 -0.008561  ...  0.013700  0.033830 -0.000301  0.038543   \n",
       "2  0.032614 -0.004093  0.147141  ...  0.015431  0.042424 -0.017514  0.008480   \n",
       "3  0.005338 -0.016251 -0.146453  ... -0.004665  0.009040 -0.021966 -0.035676   \n",
       "4  0.069833 -0.046911 -0.018658  ...  0.064636  0.068888 -0.018376  0.007812   \n",
       "\n",
       "   tfidf_95  tfidf_96  tfidf_97  tfidf_98  tfidf_99  essay_id  \n",
       "0  0.012424 -0.014837  0.003919 -0.041749  0.040429   000d118  \n",
       "1 -0.017799 -0.002648  0.017269 -0.018821  0.018258   000fe60  \n",
       "2 -0.000546  0.011427 -0.038294 -0.023080 -0.012667   001bdc0  \n",
       "3  0.003183  0.004906  0.016478 -0.034712  0.015269   0036253  \n",
       "4  0.022137 -0.010695 -0.035521 -0.007320  0.035616   0047cb3  \n",
       "\n",
       "[5 rows x 101 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oof_tfidf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_tfidf.to_csv(f\"{MODEL_OUTPUT_PATH}/oof_tfidf.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggleへのアップロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Dataset name:e-word-tfidf-tfidf, output_dir:../../trained_models/e-word-tfidf\n",
      "Starting upload for file tfidf_vectorizer_s_sl_g_p_fold2.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 737k/737k [00:01<00:00, 573kB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: tfidf_vectorizer_s_sl_g_p_fold2.pkl (737KB)\n",
      "Starting upload for file tfidf_vectorizer_s_sl_g_p_fold0.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 737k/737k [00:01<00:00, 563kB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: tfidf_vectorizer_s_sl_g_p_fold0.pkl (737KB)\n",
      "Starting upload for file tfidf_vectorizer_s_sl_g_p_fold1.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 737k/737k [00:01<00:00, 590kB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: tfidf_vectorizer_s_sl_g_p_fold1.pkl (737KB)\n",
      "Starting upload for file oof_tfidf.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35.0M/35.0M [00:02<00:00, 12.8MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: oof_tfidf.csv (35MB)\n"
     ]
    }
   ],
   "source": [
    "if UPLOAD_DATA_TO_KAGGLE:\n",
    "    import os\n",
    "    import json\n",
    "\n",
    "    from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "    def dataset_create_new(dataset_name: str, upload_dir: str):\n",
    "        # if \"_\" in dataset_name:\n",
    "        #     raise ValueError(\"datasetの名称に_の使用は禁止です\")\n",
    "        dataset_metadata = {}\n",
    "        dataset_metadata[\"id\"] = f\"sinchir0/{dataset_name}\"\n",
    "        dataset_metadata[\"licenses\"] = [{\"name\": \"CC0-1.0\"}]\n",
    "        dataset_metadata[\"title\"] = dataset_name\n",
    "        with open(os.path.join(upload_dir, \"dataset-metadata.json\"), \"w\") as f:\n",
    "            json.dump(dataset_metadata, f, indent=4)\n",
    "        api = KaggleApi()\n",
    "        api.authenticate()\n",
    "        api.dataset_create_new(folder=upload_dir, convert_to_csv=False, dir_mode=\"tar\")\n",
    "\n",
    "    print(f\"Create Dataset name:{DATASET_NAME}, output_dir:{MODEL_OUTPUT_PATH}\")\n",
    "    dataset_create_new(dataset_name=DATASET_NAME, upload_dir=MODEL_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
