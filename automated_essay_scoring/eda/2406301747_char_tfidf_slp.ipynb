{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_NAME = \"e-char-tfidf-slp\"\n",
    "MODEL_NAME = \"tfidf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"data\"\n",
    "SEED = 42\n",
    "N_FOLD = 3\n",
    "\n",
    "# path setting\n",
    "COMPETITION_NAME = \"automated_essay_scoring\"\n",
    "DATASET_NAME = f\"{EXP_NAME}-{MODEL_NAME.split('/')[-1]}\"\n",
    "MODEL_OUTPUT_PATH = f\"trained_models/{EXP_NAME}\"\n",
    "\n",
    "UPLOAD_DATA_TO_KAGGLE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_dataset_name(dataset_name: str) -> None:\n",
    "    if len(dataset_name) < 6 or len(dataset_name) > 50:\n",
    "        raise Exception(\n",
    "            f\"データセットの文字列は6~50文字にしてください。現在{len(DATASET_NAME)}文字\"\n",
    "        )\n",
    "    if \"_\" in dataset_name:\n",
    "        raise Exception(\"datasetの名称に_の使用は禁止です\")\n",
    "\n",
    "\n",
    "validate_dataset_name(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/shinichiro.saito/automated_essay_scoring/automated_essay_scoring/eda\n",
      "Local Mac!\n",
      "../../data\n",
      "/Users/shinichiro.saito/automated_essay_scoring/automated_essay_scoring/eda\n",
      "Local Mac!\n",
      "../../trained_models/e-char-tfidf-slp\n"
     ]
    }
   ],
   "source": [
    "def resolve_path(base_path: str) -> str:\n",
    "    import os\n",
    "\n",
    "    cwd = os.getcwd()\n",
    "    print(cwd)\n",
    "    if cwd == f\"/notebooks\":\n",
    "        print(\"Jupyter Kernel By VSCode!\")\n",
    "        return f\"/notebooks/{COMPETITION_NAME}/{base_path}\"\n",
    "    elif cwd == f\"/notebooks/{COMPETITION_NAME}\":\n",
    "        print(\"nohup!\")\n",
    "        return base_path\n",
    "    elif cwd == f\"/notebooks/{COMPETITION_NAME}/{COMPETITION_NAME}/exp\":\n",
    "        print(\"Jupyter Lab!\")\n",
    "        return f\"../../{base_path}\"\n",
    "    elif cwd.startswith(\"/Users\"):\n",
    "        print(\"Local Mac!\")\n",
    "        return f\"../../{base_path}\"\n",
    "    else:\n",
    "        raise Exception(\"Unknown environment\")\n",
    "\n",
    "\n",
    "DATA_PATH = resolve_path(DATA_PATH)\n",
    "print(DATA_PATH)\n",
    "MODEL_OUTPUT_PATH = resolve_path(MODEL_OUTPUT_PATH)\n",
    "print(MODEL_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(MODEL_OUTPUT_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed the same seed to all\n",
    "def seed_everything(seed: int) -> None:\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pl.read_csv(f\"{DATA_PATH}/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{DATA_PATH}/essay_id_fold_by_slp_only_train_dict.json\") as f:\n",
    "    essay_id_fold_only_train = json.load(f)\n",
    "\n",
    "train = train.with_columns(\n",
    "    pl.col(\"essay_id\")\n",
    "    .replace(essay_id_fold_only_train, return_dtype=pl.Int64)\n",
    "    .alias(\"fold\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>essay_id</th><th>full_text</th><th>score</th><th>fold</th></tr><tr><td>str</td><td>str</td><td>i64</td><td>i64</td></tr></thead><tbody><tr><td>&quot;000d118&quot;</td><td>&quot;Many people have car where the…</td><td>3</td><td>0</td></tr><tr><td>&quot;000fe60&quot;</td><td>&quot;I am a scientist at NASA that …</td><td>3</td><td>2</td></tr><tr><td>&quot;001ab80&quot;</td><td>&quot;People always wish they had th…</td><td>4</td><td>2</td></tr><tr><td>&quot;001bdc0&quot;</td><td>&quot;We all heard about Venus, the …</td><td>4</td><td>2</td></tr><tr><td>&quot;002ba53&quot;</td><td>&quot;Dear, State Senator\n",
       "\n",
       "This is a…</td><td>3</td><td>1</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 4)\n",
       "┌──────────┬─────────────────────────────────┬───────┬──────┐\n",
       "│ essay_id ┆ full_text                       ┆ score ┆ fold │\n",
       "│ ---      ┆ ---                             ┆ ---   ┆ ---  │\n",
       "│ str      ┆ str                             ┆ i64   ┆ i64  │\n",
       "╞══════════╪═════════════════════════════════╪═══════╪══════╡\n",
       "│ 000d118  ┆ Many people have car where the… ┆ 3     ┆ 0    │\n",
       "│ 000fe60  ┆ I am a scientist at NASA that … ┆ 3     ┆ 2    │\n",
       "│ 001ab80  ┆ People always wish they had th… ┆ 4     ┆ 2    │\n",
       "│ 001bdc0  ┆ We all heard about Venus, the … ┆ 4     ┆ 2    │\n",
       "│ 002ba53  ┆ Dear, State Senator             ┆ 3     ┆ 1    │\n",
       "│          ┆                                 ┆       ┆      │\n",
       "│          ┆ This is a…                      ┆       ┆      │\n",
       "└──────────┴─────────────────────────────────┴───────┴──────┘"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=lambda x: x,\n",
    "    preprocessor=lambda x: x,\n",
    "    token_pattern=None,\n",
    "    strip_accents=\"unicode\",\n",
    "    analyzer=\"word\",\n",
    "    ngram_range=(3, 6),\n",
    "    min_df=0.05,\n",
    "    max_df=0.95,\n",
    "    sublinear_tf=True,\n",
    ")\n",
    "vectorizer.fit(train[\"full_text\"])\n",
    "all_voc = vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19627"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shinichiro.saito/automated_essay_scoring/.venv/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:1381: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shinichiro.saito/automated_essay_scoring/.venv/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:1381: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shinichiro.saito/automated_essay_scoring/.venv/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:1381: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import pickle\n",
    "\n",
    "\n",
    "oofs: list[pd.DataFrame] = []\n",
    "# Cross Validationによる学習の実施\n",
    "for fold in range(N_FOLD):\n",
    "    print(f\"Start fold {fold}\")\n",
    "\n",
    "    # foldごとにtrainとvalidに分ける\n",
    "    train_fold = train.filter(pl.col(\"fold\") != fold)\n",
    "    valid_fold = train.filter(pl.col(\"fold\") == fold)\n",
    "\n",
    "    def tfidf_tokenizer(x):\n",
    "        return x\n",
    "\n",
    "    def tfidf_preprocessor(x):\n",
    "        return x\n",
    "\n",
    "    # TfidfVectorizer parameter\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        tokenizer=tfidf_tokenizer,\n",
    "        preprocessor=tfidf_preprocessor,\n",
    "        token_pattern=None,\n",
    "        strip_accents=\"unicode\",\n",
    "        analyzer=\"word\",\n",
    "        ngram_range=(3, 6),\n",
    "        min_df=0.05,\n",
    "        max_df=0.95,\n",
    "        sublinear_tf=True,\n",
    "        vocabulary=all_voc,\n",
    "    )\n",
    "\n",
    "    vectorizer.fit(train_fold[\"full_text\"])\n",
    "    valid_tfid = vectorizer.transform(valid_fold[\"full_text\"])\n",
    "\n",
    "    dense_matrix = valid_tfid.toarray()\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        dense_matrix,\n",
    "        columns=[f\"tfidf_{i}\" for i in range(len(all_voc))],\n",
    "    )\n",
    "\n",
    "    df[\"essay_id\"] = valid_fold[\"essay_id\"]\n",
    "\n",
    "    oofs.append(df)\n",
    "\n",
    "    # save vectorizer\n",
    "    with open(\n",
    "        f\"{MODEL_OUTPUT_PATH}/tfidf_vectorizer_s_sl_g_p_fold{fold}.pkl\", \"wb\"\n",
    "    ) as file:\n",
    "        pickle.dump(vectorizer, file)\n",
    "\n",
    "all_tfidf_res = pd.concat(oofs)\n",
    "\n",
    "# SVD : 次元削減\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=100)\n",
    "all_tfidf_res_reduced = svd.fit_transform(\n",
    "    all_tfidf_res[[f\"tfidf_{i}\" for i in range(len(all_voc))]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_tfidf = pd.DataFrame(\n",
    "    all_tfidf_res_reduced,\n",
    "    columns=[f\"tfidf_{i}\" for i in range(100)],\n",
    ")\n",
    "oof_tfidf[\"essay_id\"] = all_tfidf_res[\"essay_id\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf_0</th>\n",
       "      <th>tfidf_1</th>\n",
       "      <th>tfidf_2</th>\n",
       "      <th>tfidf_3</th>\n",
       "      <th>tfidf_4</th>\n",
       "      <th>tfidf_5</th>\n",
       "      <th>tfidf_6</th>\n",
       "      <th>tfidf_7</th>\n",
       "      <th>tfidf_8</th>\n",
       "      <th>tfidf_9</th>\n",
       "      <th>...</th>\n",
       "      <th>tfidf_91</th>\n",
       "      <th>tfidf_92</th>\n",
       "      <th>tfidf_93</th>\n",
       "      <th>tfidf_94</th>\n",
       "      <th>tfidf_95</th>\n",
       "      <th>tfidf_96</th>\n",
       "      <th>tfidf_97</th>\n",
       "      <th>tfidf_98</th>\n",
       "      <th>tfidf_99</th>\n",
       "      <th>essay_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.480761</td>\n",
       "      <td>-0.069906</td>\n",
       "      <td>-0.029684</td>\n",
       "      <td>-0.124607</td>\n",
       "      <td>-0.025792</td>\n",
       "      <td>0.052792</td>\n",
       "      <td>0.200360</td>\n",
       "      <td>0.129328</td>\n",
       "      <td>0.035904</td>\n",
       "      <td>0.003206</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027629</td>\n",
       "      <td>-0.070550</td>\n",
       "      <td>-0.024856</td>\n",
       "      <td>-0.004870</td>\n",
       "      <td>0.029800</td>\n",
       "      <td>-0.023274</td>\n",
       "      <td>0.024487</td>\n",
       "      <td>0.052111</td>\n",
       "      <td>-0.044755</td>\n",
       "      <td>000d118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.444141</td>\n",
       "      <td>-0.222407</td>\n",
       "      <td>0.314096</td>\n",
       "      <td>0.053528</td>\n",
       "      <td>-0.018858</td>\n",
       "      <td>-0.040603</td>\n",
       "      <td>0.038249</td>\n",
       "      <td>-0.062817</td>\n",
       "      <td>-0.034133</td>\n",
       "      <td>-0.008763</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020301</td>\n",
       "      <td>0.003739</td>\n",
       "      <td>-0.043478</td>\n",
       "      <td>0.019699</td>\n",
       "      <td>0.009077</td>\n",
       "      <td>0.029467</td>\n",
       "      <td>-0.036117</td>\n",
       "      <td>-0.007297</td>\n",
       "      <td>-0.064364</td>\n",
       "      <td>0030e86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.342362</td>\n",
       "      <td>-0.046622</td>\n",
       "      <td>-0.147968</td>\n",
       "      <td>0.244566</td>\n",
       "      <td>-0.012355</td>\n",
       "      <td>-0.011902</td>\n",
       "      <td>-0.012998</td>\n",
       "      <td>-0.006787</td>\n",
       "      <td>-0.221300</td>\n",
       "      <td>-0.024329</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008213</td>\n",
       "      <td>0.006503</td>\n",
       "      <td>-0.011601</td>\n",
       "      <td>-0.022842</td>\n",
       "      <td>0.007145</td>\n",
       "      <td>0.059649</td>\n",
       "      <td>-0.029824</td>\n",
       "      <td>-0.032984</td>\n",
       "      <td>-0.004266</td>\n",
       "      <td>0033037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.384236</td>\n",
       "      <td>-0.070743</td>\n",
       "      <td>-0.070861</td>\n",
       "      <td>-0.077383</td>\n",
       "      <td>0.086623</td>\n",
       "      <td>0.516206</td>\n",
       "      <td>-0.114914</td>\n",
       "      <td>-0.098427</td>\n",
       "      <td>0.015945</td>\n",
       "      <td>-0.005567</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000737</td>\n",
       "      <td>0.009360</td>\n",
       "      <td>-0.014728</td>\n",
       "      <td>-0.006816</td>\n",
       "      <td>0.008347</td>\n",
       "      <td>0.000849</td>\n",
       "      <td>-0.006443</td>\n",
       "      <td>-0.018661</td>\n",
       "      <td>-0.020186</td>\n",
       "      <td>0033bf4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.379743</td>\n",
       "      <td>-0.048274</td>\n",
       "      <td>-0.117454</td>\n",
       "      <td>-0.088526</td>\n",
       "      <td>-0.040347</td>\n",
       "      <td>-0.076423</td>\n",
       "      <td>-0.109063</td>\n",
       "      <td>0.073225</td>\n",
       "      <td>0.028899</td>\n",
       "      <td>-0.038056</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030268</td>\n",
       "      <td>-0.036217</td>\n",
       "      <td>0.027628</td>\n",
       "      <td>0.027753</td>\n",
       "      <td>0.012181</td>\n",
       "      <td>-0.019016</td>\n",
       "      <td>0.023670</td>\n",
       "      <td>-0.006825</td>\n",
       "      <td>-0.043504</td>\n",
       "      <td>004229b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    tfidf_0   tfidf_1   tfidf_2   tfidf_3   tfidf_4   tfidf_5   tfidf_6  \\\n",
       "0  0.480761 -0.069906 -0.029684 -0.124607 -0.025792  0.052792  0.200360   \n",
       "1  0.444141 -0.222407  0.314096  0.053528 -0.018858 -0.040603  0.038249   \n",
       "2  0.342362 -0.046622 -0.147968  0.244566 -0.012355 -0.011902 -0.012998   \n",
       "3  0.384236 -0.070743 -0.070861 -0.077383  0.086623  0.516206 -0.114914   \n",
       "4  0.379743 -0.048274 -0.117454 -0.088526 -0.040347 -0.076423 -0.109063   \n",
       "\n",
       "    tfidf_7   tfidf_8   tfidf_9  ...  tfidf_91  tfidf_92  tfidf_93  tfidf_94  \\\n",
       "0  0.129328  0.035904  0.003206  ...  0.027629 -0.070550 -0.024856 -0.004870   \n",
       "1 -0.062817 -0.034133 -0.008763  ... -0.020301  0.003739 -0.043478  0.019699   \n",
       "2 -0.006787 -0.221300 -0.024329  ... -0.008213  0.006503 -0.011601 -0.022842   \n",
       "3 -0.098427  0.015945 -0.005567  ...  0.000737  0.009360 -0.014728 -0.006816   \n",
       "4  0.073225  0.028899 -0.038056  ...  0.030268 -0.036217  0.027628  0.027753   \n",
       "\n",
       "   tfidf_95  tfidf_96  tfidf_97  tfidf_98  tfidf_99  essay_id  \n",
       "0  0.029800 -0.023274  0.024487  0.052111 -0.044755   000d118  \n",
       "1  0.009077  0.029467 -0.036117 -0.007297 -0.064364   0030e86  \n",
       "2  0.007145  0.059649 -0.029824 -0.032984 -0.004266   0033037  \n",
       "3  0.008347  0.000849 -0.006443 -0.018661 -0.020186   0033bf4  \n",
       "4  0.012181 -0.019016  0.023670 -0.006825 -0.043504   004229b  \n",
       "\n",
       "[5 rows x 101 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oof_tfidf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_tfidf.to_csv(f\"{MODEL_OUTPUT_PATH}/oof_tfidf.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggleへのアップロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Dataset name:e-char-tfidf-slp-tfidf, output_dir:../../trained_models/e-char-tfidf-slp\n",
      "Starting upload for file tfidf_vectorizer_s_sl_g_p_fold2.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 737k/737k [00:01<00:00, 584kB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: tfidf_vectorizer_s_sl_g_p_fold2.pkl (737KB)\n",
      "Starting upload for file tfidf_vectorizer_s_sl_g_p_fold0.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 737k/737k [00:01<00:00, 629kB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: tfidf_vectorizer_s_sl_g_p_fold0.pkl (737KB)\n",
      "Starting upload for file tfidf_vectorizer_s_sl_g_p_fold1.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 737k/737k [00:01<00:00, 571kB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: tfidf_vectorizer_s_sl_g_p_fold1.pkl (737KB)\n",
      "Starting upload for file oof_tfidf.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34.9M/34.9M [00:02<00:00, 13.1MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: oof_tfidf.csv (35MB)\n"
     ]
    }
   ],
   "source": [
    "if UPLOAD_DATA_TO_KAGGLE:\n",
    "    import os\n",
    "    import json\n",
    "\n",
    "    from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "    def dataset_create_new(dataset_name: str, upload_dir: str):\n",
    "        # if \"_\" in dataset_name:\n",
    "        #     raise ValueError(\"datasetの名称に_の使用は禁止です\")\n",
    "        dataset_metadata = {}\n",
    "        dataset_metadata[\"id\"] = f\"sinchir0/{dataset_name}\"\n",
    "        dataset_metadata[\"licenses\"] = [{\"name\": \"CC0-1.0\"}]\n",
    "        dataset_metadata[\"title\"] = dataset_name\n",
    "        with open(os.path.join(upload_dir, \"dataset-metadata.json\"), \"w\") as f:\n",
    "            json.dump(dataset_metadata, f, indent=4)\n",
    "        api = KaggleApi()\n",
    "        api.authenticate()\n",
    "        api.dataset_create_new(folder=upload_dir, convert_to_csv=False, dir_mode=\"tar\")\n",
    "\n",
    "    print(f\"Create Dataset name:{DATASET_NAME}, output_dir:{MODEL_OUTPUT_PATH}\")\n",
    "    dataset_create_new(dataset_name=DATASET_NAME, upload_dir=MODEL_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
