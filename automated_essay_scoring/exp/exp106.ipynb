{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 目的\n",
    "テキスト特徴量を諸々追加する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path setting\n",
    "EXP_NAME = \"e106-text-featrue\"\n",
    "MODEL_NAME = \"lightgbm\"\n",
    "COMPETITION_NAME = \"automated_essay_scoring\"\n",
    "\n",
    "DATA_PATH = \"data\"\n",
    "DATASET_NAME = f\"{EXP_NAME}-{MODEL_NAME.split('/')[-1]}\"\n",
    "MODEL_OUTPUT_PATH = f\"trained_models/{EXP_NAME}\"\n",
    "\n",
    "N_FOLD = 3\n",
    "\n",
    "# experiment parameter\n",
    "DEBUG = False\n",
    "TRAINING = True\n",
    "UPLOAD_DATA_TO_S3 = False\n",
    "UPLOAD_DATA_TO_KAGGLE = False\n",
    "WANDB = True\n",
    "\n",
    "# model parameter\n",
    "SEED = 42\n",
    "EPOCH = 4\n",
    "LR = 2e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_dataset_name(dataset_name: str) -> None:\n",
    "    if len(dataset_name) < 6 or len(dataset_name) > 50:\n",
    "        raise Exception(\n",
    "            f\"データセットの文字列は6~50文字にしてください。現在{len(DATASET_NAME)}文字\"\n",
    "        )\n",
    "    if \"_\" in dataset_name:\n",
    "        raise Exception(\"datasetの名称に_の使用は禁止です\")\n",
    "\n",
    "\n",
    "validate_dataset_name(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.7\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/shinichiro.saito/automated_essay_scoring/automated_essay_scoring/exp\n",
      "Local Mac!\n",
      "../../data\n",
      "/Users/shinichiro.saito/automated_essay_scoring/automated_essay_scoring/exp\n",
      "Local Mac!\n",
      "../../trained_models/e106-text-featrue\n"
     ]
    }
   ],
   "source": [
    "def resolve_path(base_path: str) -> str:\n",
    "    import os\n",
    "\n",
    "    cwd = os.getcwd()\n",
    "    print(cwd)\n",
    "    if cwd == f\"/notebooks\":\n",
    "        print(\"Jupyter Kernel By VSCode!\")\n",
    "        return f\"/notebooks/{COMPETITION_NAME}/{base_path}\"\n",
    "    elif cwd == f\"/notebooks/{COMPETITION_NAME}\":\n",
    "        print(\"nohup!\")\n",
    "        return base_path\n",
    "    elif cwd == f\"/notebooks/{COMPETITION_NAME}/{COMPETITION_NAME}/exp\":\n",
    "        print(\"Jupyter Lab!\")\n",
    "        return f\"../../{base_path}\"\n",
    "    elif cwd.startswith(\"/Users\"):\n",
    "        print(\"Local Mac!\")\n",
    "        return f\"../../{base_path}\"\n",
    "    else:\n",
    "        raise Exception(\"Unknown environment\")\n",
    "\n",
    "\n",
    "DATA_PATH = resolve_path(DATA_PATH)\n",
    "print(DATA_PATH)\n",
    "MODEL_OUTPUT_PATH = resolve_path(MODEL_OUTPUT_PATH)\n",
    "print(MODEL_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(MODEL_OUTPUT_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import torch\n",
    "import json\n",
    "import re\n",
    "import spacy\n",
    "import polars as pl\n",
    "import lightgbm as lgb\n",
    "from functools import partial\n",
    "from lightgbm import log_evaluation, early_stopping\n",
    "import wandb\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed the same seed to all\n",
    "def seed_everything(seed: int) -> None:\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(f\"{DATA_PATH}/.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(f\"{DATA_PATH}/essay_id_spelling_errors_cnt.json\", \"r\") as f:\n",
    "with open(f\"{DATA_PATH}/essay_id_spelling_errors_cnt_pyspell.json\", \"r\") as f:\n",
    "    essay_id_spelling_errors_cnt = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:o4rl72h3) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">e106-text-featrue</strong> at: <a href='https://wandb.ai/sinchir0/automated_essay_scoring/runs/o4rl72h3' target=\"_blank\">https://wandb.ai/sinchir0/automated_essay_scoring/runs/o4rl72h3</a><br/> View project at: <a href='https://wandb.ai/sinchir0/automated_essay_scoring' target=\"_blank\">https://wandb.ai/sinchir0/automated_essay_scoring</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240627_071812-o4rl72h3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:o4rl72h3). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/shinichiro.saito/automated_essay_scoring/automated_essay_scoring/exp/wandb/run-20240627_072222-j6x2chc3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sinchir0/automated_essay_scoring/runs/j6x2chc3' target=\"_blank\">e106-text-featrue</a></strong> to <a href='https://wandb.ai/sinchir0/automated_essay_scoring' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sinchir0/automated_essay_scoring' target=\"_blank\">https://wandb.ai/sinchir0/automated_essay_scoring</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sinchir0/automated_essay_scoring/runs/j6x2chc3' target=\"_blank\">https://wandb.ai/sinchir0/automated_essay_scoring/runs/j6x2chc3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'wandb'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if WANDB:\n",
    "    wandb.login(key=os.environ[\"WANDB_API_KEY\"])\n",
    "    wandb.init(project=\"automated_essay_scoring\", name=EXP_NAME)\n",
    "    REPORT_TO = \"wandb\"\n",
    "else:\n",
    "    REPORT_TO = \"none\"\n",
    "\n",
    "REPORT_TO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pl.read_csv(f\"{DATA_PATH}/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>essay_id</th><th>full_text</th><th>score</th></tr><tr><td>str</td><td>str</td><td>i64</td></tr></thead><tbody><tr><td>&quot;000d118&quot;</td><td>&quot;Many people have car where the…</td><td>3</td></tr><tr><td>&quot;000fe60&quot;</td><td>&quot;I am a scientist at NASA that …</td><td>3</td></tr><tr><td>&quot;001ab80&quot;</td><td>&quot;People always wish they had th…</td><td>4</td></tr><tr><td>&quot;001bdc0&quot;</td><td>&quot;We all heard about Venus, the …</td><td>4</td></tr><tr><td>&quot;002ba53&quot;</td><td>&quot;Dear, State Senator\n",
       "\n",
       "This is a…</td><td>3</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 3)\n",
       "┌──────────┬─────────────────────────────────┬───────┐\n",
       "│ essay_id ┆ full_text                       ┆ score │\n",
       "│ ---      ┆ ---                             ┆ ---   │\n",
       "│ str      ┆ str                             ┆ i64   │\n",
       "╞══════════╪═════════════════════════════════╪═══════╡\n",
       "│ 000d118  ┆ Many people have car where the… ┆ 3     │\n",
       "│ 000fe60  ┆ I am a scientist at NASA that … ┆ 3     │\n",
       "│ 001ab80  ┆ People always wish they had th… ┆ 4     │\n",
       "│ 001bdc0  ┆ We all heard about Venus, the … ┆ 4     │\n",
       "│ 002ba53  ┆ Dear, State Senator             ┆ 3     │\n",
       "│          ┆                                 ┆       │\n",
       "│          ┆ This is a…                      ┆       │\n",
       "└──────────┴─────────────────────────────────┴───────┘"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{DATA_PATH}/essay_id_fold_by_s_sl_g_p_only_train_dict.json\") as f:\n",
    "    essay_id_fold_only_train = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.with_columns(\n",
    "    pl.col(\"essay_id\")\n",
    "    .replace(essay_id_fold_only_train, return_dtype=pl.Int64)\n",
    "    .alias(\"fold\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_e053 = pl.read_csv(f\"{DATA_PATH}/e053-cv-w-oof/oof.csv\").rename(\n",
    "    {\"valid_pred\": \"e053_xsmall_oof\"}\n",
    ")\n",
    "train = train.join(oof_e053, on=\"essay_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_e054 = pl.read_csv(f\"{DATA_PATH}/e054-cv-w-oof/oof.csv\").rename(\n",
    "    {\"valid_pred\": \"e054_small_oof\"}\n",
    ")\n",
    "train = train.join(oof_e054, on=\"essay_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_e055 = pl.read_csv(f\"{DATA_PATH}/e055-cv-w-oof/oof.csv\").rename(\n",
    "    {\"valid_pred\": \"e055_base_oof\"}\n",
    ")\n",
    "train = train.join(oof_e055, on=\"essay_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_e056 = pl.read_csv(f\"{DATA_PATH}/e056-cv-w-oof/oof.csv\").rename(\n",
    "    {\"valid_pred\": \"e056_large_oof\"}\n",
    ")\n",
    "train = train.join(oof_e056, on=\"essay_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 12)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>essay_id</th><th>full_text</th><th>score</th><th>fold</th><th>e053_xsmall_oof</th><th>e054_small_oof</th><th>e055_base_oof</th><th>e056_large_oof</th><th>text_length</th><th>word_length</th><th>spelling_errors_cnt</th><th>rate_spelling_errors_per_word</th></tr><tr><td>str</td><td>str</td><td>i64</td><td>i64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>u32</td><td>i64</td><td>i64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;000d118&quot;</td><td>&quot;Many people have car where the…</td><td>3</td><td>0</td><td>2.59375</td><td>2.4804688</td><td>2.4941406</td><td>2.3769531</td><td>2677</td><td>498</td><td>22</td><td>0.044177</td></tr><tr><td>&quot;000fe60&quot;</td><td>&quot;I am a scientist at NASA that …</td><td>3</td><td>0</td><td>2.5761719</td><td>2.2910156</td><td>2.3007812</td><td>2.7050781</td><td>1669</td><td>332</td><td>5</td><td>0.01506</td></tr><tr><td>&quot;001ab80&quot;</td><td>&quot;People always wish they had th…</td><td>4</td><td>1</td><td>4.4882812</td><td>4.1523438</td><td>4.53125</td><td>4.4921875</td><td>3077</td><td>550</td><td>7</td><td>0.012727</td></tr><tr><td>&quot;001bdc0&quot;</td><td>&quot;We all heard about Venus, the …</td><td>4</td><td>0</td><td>3.4960938</td><td>3.3242188</td><td>3.0800781</td><td>3.2382812</td><td>2701</td><td>451</td><td>6</td><td>0.013304</td></tr><tr><td>&quot;002ba53&quot;</td><td>&quot;Dear, State Senator\n",
       "\n",
       "This is a…</td><td>3</td><td>2</td><td>2.8261719</td><td>2.9257812</td><td>3.0273438</td><td>2.7851562</td><td>2208</td><td>373</td><td>10</td><td>0.02681</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 12)\n",
       "┌──────────┬─────────────┬───────┬──────┬───┬─────────────┬─────────────┬─────────────┬────────────┐\n",
       "│ essay_id ┆ full_text   ┆ score ┆ fold ┆ … ┆ text_length ┆ word_length ┆ spelling_er ┆ rate_spell │\n",
       "│ ---      ┆ ---         ┆ ---   ┆ ---  ┆   ┆ ---         ┆ ---         ┆ rors_cnt    ┆ ing_errors │\n",
       "│ str      ┆ str         ┆ i64   ┆ i64  ┆   ┆ u32         ┆ i64         ┆ ---         ┆ _per_word  │\n",
       "│          ┆             ┆       ┆      ┆   ┆             ┆             ┆ i64         ┆ ---        │\n",
       "│          ┆             ┆       ┆      ┆   ┆             ┆             ┆             ┆ f64        │\n",
       "╞══════════╪═════════════╪═══════╪══════╪═══╪═════════════╪═════════════╪═════════════╪════════════╡\n",
       "│ 000d118  ┆ Many people ┆ 3     ┆ 0    ┆ … ┆ 2677        ┆ 498         ┆ 22          ┆ 0.044177   │\n",
       "│          ┆ have car    ┆       ┆      ┆   ┆             ┆             ┆             ┆            │\n",
       "│          ┆ where the…  ┆       ┆      ┆   ┆             ┆             ┆             ┆            │\n",
       "│ 000fe60  ┆ I am a      ┆ 3     ┆ 0    ┆ … ┆ 1669        ┆ 332         ┆ 5           ┆ 0.01506    │\n",
       "│          ┆ scientist   ┆       ┆      ┆   ┆             ┆             ┆             ┆            │\n",
       "│          ┆ at NASA     ┆       ┆      ┆   ┆             ┆             ┆             ┆            │\n",
       "│          ┆ that …      ┆       ┆      ┆   ┆             ┆             ┆             ┆            │\n",
       "│ 001ab80  ┆ People      ┆ 4     ┆ 1    ┆ … ┆ 3077        ┆ 550         ┆ 7           ┆ 0.012727   │\n",
       "│          ┆ always wish ┆       ┆      ┆   ┆             ┆             ┆             ┆            │\n",
       "│          ┆ they had    ┆       ┆      ┆   ┆             ┆             ┆             ┆            │\n",
       "│          ┆ th…         ┆       ┆      ┆   ┆             ┆             ┆             ┆            │\n",
       "│ 001bdc0  ┆ We all      ┆ 4     ┆ 0    ┆ … ┆ 2701        ┆ 451         ┆ 6           ┆ 0.013304   │\n",
       "│          ┆ heard about ┆       ┆      ┆   ┆             ┆             ┆             ┆            │\n",
       "│          ┆ Venus, the  ┆       ┆      ┆   ┆             ┆             ┆             ┆            │\n",
       "│          ┆ …           ┆       ┆      ┆   ┆             ┆             ┆             ┆            │\n",
       "│ 002ba53  ┆ Dear, State ┆ 3     ┆ 2    ┆ … ┆ 2208        ┆ 373         ┆ 10          ┆ 0.02681    │\n",
       "│          ┆ Senator     ┆       ┆      ┆   ┆             ┆             ┆             ┆            │\n",
       "│          ┆             ┆       ┆      ┆   ┆             ┆             ┆             ┆            │\n",
       "│          ┆ This is a…  ┆       ┆      ┆   ┆             ┆             ┆             ┆            │\n",
       "└──────────┴─────────────┴───────┴──────┴───┴─────────────┴─────────────┴─────────────┴────────────┘"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_text_length() -> pl.Expr:\n",
    "    return pl.col(\"full_text\").str.len_chars().alias(\"text_length\")\n",
    "\n",
    "\n",
    "def get_word_length() -> pl.Expr:\n",
    "    return (\n",
    "        pl.col(\"full_text\")\n",
    "        .map_elements(lambda x: len(x.split()), return_dtype=pl.Int64)\n",
    "        .alias(\"word_length\")\n",
    "    )\n",
    "\n",
    "\n",
    "def count_spelling_errors() -> pl.Expr:\n",
    "    return (\n",
    "        pl.col(\"essay_id\")\n",
    "        .replace(essay_id_spelling_errors_cnt, return_dtype=pl.Int64)\n",
    "        .alias(\"spelling_errors_cnt\")\n",
    "    )\n",
    "\n",
    "\n",
    "def rate_spelling_errors_per_word() -> pl.Expr:\n",
    "    return (pl.col(\"spelling_errors_cnt\") / pl.col(\"word_length\")).alias(\n",
    "        \"rate_spelling_errors_per_word\"\n",
    "    )\n",
    "\n",
    "\n",
    "train = train.with_columns(\n",
    "    get_text_length(), get_word_length(), count_spelling_errors()\n",
    ").with_columns(rate_spelling_errors_per_word())\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def textstat_features(text):\n",
    "    features = {}\n",
    "    features[\"flesch_reading_ease\"] = textstat.flesch_reading_ease(text)\n",
    "    features[\"flesch_kincaid_grade\"] = textstat.flesch_kincaid_grade(text)\n",
    "    features[\"smog_index\"] = textstat.smog_index(text)\n",
    "    features[\"coleman_liau_index\"] = textstat.coleman_liau_index(text)\n",
    "    features[\"automated_readability_index\"] = textstat.automated_readability_index(text)\n",
    "    features[\"dale_chall_readability_score\"] = textstat.dale_chall_readability_score(\n",
    "        text\n",
    "    )\n",
    "    features[\"difficult_words\"] = textstat.difficult_words(text)\n",
    "    features[\"linsear_write_formula\"] = textstat.linsear_write_formula(text)\n",
    "    features[\"gunning_fog\"] = textstat.gunning_fog(text)\n",
    "    features[\"text_standard\"] = textstat.text_standard(text, float_output=True)\n",
    "    features[\"spache_readability\"] = textstat.spache_readability(text)\n",
    "    features[\"mcalpine_eflaw\"] = textstat.mcalpine_eflaw(text)\n",
    "    features[\"reading_time\"] = textstat.reading_time(text)\n",
    "    features[\"syllable_count\"] = textstat.syllable_count(text)\n",
    "    features[\"lexicon_count\"] = textstat.lexicon_count(text)\n",
    "    features[\"monosyllabcount\"] = textstat.monosyllabcount(text)\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "train_text_stat_df = pl.DataFrame(\n",
    "    train[\"full_text\"].to_pandas().apply(textstat_features).tolist()\n",
    ")\n",
    "\n",
    "train = pl.concat([train, train_text_stat_df], how=\"horizontal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17307/17307 [21:16<00:00, 13.56it/s]\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "# nltk.download()\n",
    "\n",
    "\n",
    "def extract_linguistic_features(text):\n",
    "\n",
    "    doc = nlp(text)\n",
    "    features = {}\n",
    "\n",
    "    # NER Features\n",
    "    entity_counts = {\n",
    "        \"GPE\": 0,\n",
    "        \"PERCENT\": 0,\n",
    "        \"NORP\": 0,\n",
    "        \"ORG\": 0,\n",
    "        \"CARDINAL\": 0,\n",
    "        \"MONEY\": 0,\n",
    "        \"DATE\": 0,\n",
    "        \"LOC\": 0,\n",
    "        \"PERSON\": 0,\n",
    "        \"QUANTITY\": 0,\n",
    "        \"EVENT\": 0,\n",
    "        \"ORDINAL\": 0,\n",
    "        \"WORK_OF_ART\": 0,\n",
    "        \"LAW\": 0,\n",
    "        \"PRODUCT\": 0,\n",
    "        \"TIME\": 0,\n",
    "        \"FAC\": 0,\n",
    "        \"LANGUAGE\": 0,\n",
    "    }\n",
    "    for entity in doc.ents:\n",
    "        if entity.label_ in entity_counts:\n",
    "            entity_counts[entity.label_] += 1\n",
    "    features[\"NER_Features\"] = entity_counts\n",
    "\n",
    "    # POS Features\n",
    "    pos_counts = {\n",
    "        \"ADJ\": 0,\n",
    "        \"NOUN\": 0,\n",
    "        \"VERB\": 0,\n",
    "        \"SCONJ\": 0,\n",
    "        \"PRON\": 0,\n",
    "        \"PUNCT\": 0,\n",
    "        \"DET\": 0,\n",
    "        \"AUX\": 0,\n",
    "        \"PART\": 0,\n",
    "        \"ADP\": 0,\n",
    "        \"SPACE\": 0,\n",
    "        \"CCONJ\": 0,\n",
    "        \"PROPN\": 0,\n",
    "        \"NUM\": 0,\n",
    "        \"ADV\": 0,\n",
    "        \"SYM\": 0,\n",
    "        \"INTJ\": 0,\n",
    "        \"X\": 0,\n",
    "    }\n",
    "    for token in doc:\n",
    "        if token.pos_ in pos_counts:\n",
    "            pos_counts[token.pos_] += 1\n",
    "    features[\"POS_Features\"] = pos_counts\n",
    "\n",
    "    # tag Features\n",
    "    tags = {\n",
    "        \"RB\": 0,\n",
    "        \"-RRB-\": 0,\n",
    "        \"PRP$\": 0,\n",
    "        \"JJ\": 0,\n",
    "        \"TO\": 0,\n",
    "        \"VBP\": 0,\n",
    "        \"JJS\": 0,\n",
    "        \"DT\": 0,\n",
    "        \"''\": 0,\n",
    "        \"UH\": 0,\n",
    "        \"RBS\": 0,\n",
    "        \"WRB\": 0,\n",
    "        \".\": 0,\n",
    "        \"HYPH\": 0,\n",
    "        \"XX\": 0,\n",
    "        \"``\": 0,\n",
    "        \"SYM\": 0,\n",
    "        \"VB\": 0,\n",
    "        \"VBN\": 0,\n",
    "        \"WP\": 0,\n",
    "        \"CC\": 0,\n",
    "        \"LS\": 0,\n",
    "        \"POS\": 0,\n",
    "        \"NN\": 0,\n",
    "        \",\": 0,\n",
    "        \"NNPS\": 0,\n",
    "        \"RP\": 0,\n",
    "        \":\": 0,\n",
    "        \"$\": 0,\n",
    "        \"PDT\": 0,\n",
    "        \"VBZ\": 0,\n",
    "        \"VBD\": 0,\n",
    "        \"JJR\": 0,\n",
    "        \"-LRB-\": 0,\n",
    "        \"IN\": 0,\n",
    "        \"RBR\": 0,\n",
    "        \"WDT\": 0,\n",
    "        \"EX\": 0,\n",
    "        \"MD\": 0,\n",
    "        \"_SP\": 0,\n",
    "        \"NNP\": 0,\n",
    "        \"CD\": 0,\n",
    "        \"VBG\": 0,\n",
    "        \"NNS\": 0,\n",
    "        \"PRP\": 0,\n",
    "    }\n",
    "\n",
    "    for token in doc:\n",
    "        if token.tag_ in tags:\n",
    "            tags[token.tag_] += 1\n",
    "    features[\"tag_Features\"] = tags\n",
    "\n",
    "    # tense features\n",
    "    tenses = [i.morph.get(\"Tense\") for i in doc]\n",
    "    tenses = [i[0] for i in tenses if i]\n",
    "    tense_counts = Counter(tenses)\n",
    "    features[\"past_tense_ratio\"] = tense_counts.get(\"Past\", 0) / (\n",
    "        tense_counts.get(\"Pres\", 0) + tense_counts.get(\"Past\", 0) + 1e-5\n",
    "    )\n",
    "    features[\"present_tense_ratio\"] = tense_counts.get(\"Pres\", 0) / (\n",
    "        tense_counts.get(\"Pres\", 0) + tense_counts.get(\"Past\", 0) + 1e-5\n",
    "    )\n",
    "\n",
    "    # len features\n",
    "\n",
    "    features[\"word_count\"] = len(doc)\n",
    "    features[\"sentence_count\"] = len([sentence for sentence in doc.sents])\n",
    "    features[\"words_per_sentence\"] = features[\"word_count\"] / features[\"sentence_count\"]\n",
    "    features[\"std_words_per_sentence\"] = np.std(\n",
    "        [len(sentence) for sentence in doc.sents]\n",
    "    )\n",
    "\n",
    "    features[\"unique_words\"] = len(set([token.text for token in doc]))\n",
    "    features[\"lexical_diversity\"] = features[\"unique_words\"] / features[\"word_count\"]\n",
    "\n",
    "    paragraph = text.split(\"\\n\\n\")\n",
    "\n",
    "    features[\"paragraph_count\"] = len(paragraph)\n",
    "\n",
    "    features[\"avg_chars_by_paragraph\"] = np.mean(\n",
    "        [len(paragraph) for paragraph in paragraph]\n",
    "    )\n",
    "    features[\"avg_words_by_paragraph\"] = np.mean(\n",
    "        [len(nltk.word_tokenize(paragraph)) for paragraph in paragraph]\n",
    "    )\n",
    "    features[\"avg_sentences_by_paragraph\"] = np.mean(\n",
    "        [len(nltk.sent_tokenize(paragraph)) for paragraph in paragraph]\n",
    "    )\n",
    "\n",
    "    # sentiment features\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "    compound_scores, negative_scores, positive_scores, neutral_scores = [], [], [], []\n",
    "    for sentence in sentences:\n",
    "        scores = analyzer.polarity_scores(sentence)\n",
    "        compound_scores.append(scores[\"compound\"])\n",
    "        negative_scores.append(scores[\"neg\"])\n",
    "        positive_scores.append(scores[\"pos\"])\n",
    "        neutral_scores.append(scores[\"neu\"])\n",
    "\n",
    "    features[\"mean_compound\"] = np.mean(compound_scores)\n",
    "    features[\"mean_negative\"] = np.mean(negative_scores)\n",
    "    features[\"mean_positive\"] = np.mean(positive_scores)\n",
    "    features[\"mean_neutral\"] = np.mean(neutral_scores)\n",
    "\n",
    "    features[\"std_compound\"] = np.std(compound_scores)\n",
    "    features[\"std_negative\"] = np.std(negative_scores)\n",
    "    features[\"std_positive\"] = np.std(positive_scores)\n",
    "    features[\"std_neutral\"] = np.std(neutral_scores)\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "train_linguistic = pl.DataFrame(\n",
    "    pd.json_normalize(\n",
    "        train.to_pandas()[\"full_text\"].progress_apply(extract_linguistic_features)\n",
    "    )\n",
    ")\n",
    "\n",
    "train = pl.concat([train, train_linguistic], how=\"horizontal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "train_linguistic.write_csv(f\"{DATA_PATH}/train_linguistic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_df = pd.read_csv(\"/kaggle/input/feedback-data/feedback_data.csv\")\n",
    "\n",
    "feed_embeds = []\n",
    "\n",
    "merged_embeds = []\n",
    "\n",
    "test_embeds = []\n",
    "\n",
    "for i in range(5):\n",
    "    model_path = (\n",
    "        f\"/kaggle/input/sent-debsmall/deberta_small_trained/temp_fold{i}_checkpoints\"\n",
    "    )\n",
    "    word_embedding_model = models.Transformer(model_path, max_seq_length=1024)\n",
    "    pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "    model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "\n",
    "    model.half()\n",
    "    model = model.to(\"cuda\")\n",
    "\n",
    "    feed_custom_embeddings_train = model.encode(\n",
    "        feedback_df.loc[:, \"full_text\"].values,\n",
    "        device=\"cuda\",\n",
    "        show_progress_bar=True,\n",
    "        normalize_embeddings=True,\n",
    "    )\n",
    "\n",
    "    feed_embeds.append(feed_custom_embeddings_train)\n",
    "\n",
    "    merged_custom_embeddings = model.encode(\n",
    "        train.loc[:, \"full_text\"].values,\n",
    "        device=\"cuda\",\n",
    "        show_progress_bar=True,\n",
    "        normalize_embeddings=True,\n",
    "    )\n",
    "\n",
    "    merged_embeds.append(merged_custom_embeddings)\n",
    "\n",
    "    test_custom_embeddings = model.encode(\n",
    "        test.loc[:, \"full_text\"].values,\n",
    "        device=\"cuda\",\n",
    "        show_progress_bar=True,\n",
    "        normalize_embeddings=True,\n",
    "    )\n",
    "\n",
    "    test_embeds.append(test_custom_embeddings)\n",
    "\n",
    "feed_embeds = np.mean(feed_embeds, axis=0)\n",
    "merged_embeds = np.mean(merged_embeds, axis=0)\n",
    "test_embeds = np.mean(test_embeds, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_COL = \"score\"\n",
    "USE_COL = [\n",
    "    \"e053_xsmall_oof\",\n",
    "    \"e054_small_oof\",\n",
    "    \"e055_base_oof\",\n",
    "    \"e056_large_oof\",\n",
    "    \"text_length\",\n",
    "    \"word_length\",\n",
    "    \"spelling_errors_cnt\",\n",
    "    \"rate_spelling_errors_per_word\",\n",
    "]\n",
    "\n",
    "USE_COL += [\n",
    "    \"flesch_reading_ease\",\n",
    "    \"flesch_kincaid_grade\",\n",
    "    \"smog_index\",\n",
    "    \"coleman_liau_index\",\n",
    "    \"automated_readability_index\",\n",
    "    \"dale_chall_readability_score\",\n",
    "    \"difficult_words\",\n",
    "    \"linsear_write_formula\",\n",
    "    \"gunning_fog\",\n",
    "    \"text_standard\",\n",
    "    \"spache_readability\",\n",
    "    \"mcalpine_eflaw\",\n",
    "    \"reading_time\",\n",
    "    \"syllable_count\",\n",
    "    \"lexicon_count\",\n",
    "    \"monosyllabcount\",\n",
    "]\n",
    "\n",
    "USE_COL += [\n",
    "    \"past_tense_ratio\",\n",
    "    \"present_tense_ratio\",\n",
    "    \"word_count\",\n",
    "    \"sentence_count\",\n",
    "    \"words_per_sentence\",\n",
    "    \"std_words_per_sentence\",\n",
    "    \"unique_words\",\n",
    "    \"lexical_diversity\",\n",
    "    \"paragraph_count\",\n",
    "    \"avg_chars_by_paragraph\",\n",
    "    \"avg_words_by_paragraph\",\n",
    "    \"avg_sentences_by_paragraph\",\n",
    "    \"mean_compound\",\n",
    "    \"mean_negative\",\n",
    "    \"mean_positive\",\n",
    "    \"mean_neutral\",\n",
    "    \"std_compound\",\n",
    "    \"std_negative\",\n",
    "    \"std_positive\",\n",
    "    \"std_neutral\",\n",
    "    \"NER_Features.GPE\",\n",
    "    \"NER_Features.PERCENT\",\n",
    "    \"NER_Features.NORP\",\n",
    "    \"NER_Features.ORG\",\n",
    "    \"NER_Features.CARDINAL\",\n",
    "    \"NER_Features.MONEY\",\n",
    "    \"NER_Features.DATE\",\n",
    "    \"NER_Features.LOC\",\n",
    "    \"NER_Features.PERSON\",\n",
    "    \"NER_Features.QUANTITY\",\n",
    "    \"NER_Features.EVENT\",\n",
    "    \"NER_Features.ORDINAL\",\n",
    "    \"NER_Features.WORK_OF_ART\",\n",
    "    \"NER_Features.LAW\",\n",
    "    \"NER_Features.PRODUCT\",\n",
    "    \"NER_Features.TIME\",\n",
    "    \"NER_Features.FAC\",\n",
    "    \"NER_Features.LANGUAGE\",\n",
    "    \"POS_Features.ADJ\",\n",
    "    \"POS_Features.NOUN\",\n",
    "    \"POS_Features.VERB\",\n",
    "    \"POS_Features.SCONJ\",\n",
    "    \"POS_Features.PRON\",\n",
    "    \"POS_Features.PUNCT\",\n",
    "    \"POS_Features.DET\",\n",
    "    \"POS_Features.AUX\",\n",
    "    \"POS_Features.PART\",\n",
    "    \"POS_Features.ADP\",\n",
    "    \"POS_Features.SPACE\",\n",
    "    \"POS_Features.CCONJ\",\n",
    "    \"POS_Features.PROPN\",\n",
    "    \"POS_Features.NUM\",\n",
    "    \"POS_Features.ADV\",\n",
    "    \"POS_Features.SYM\",\n",
    "    \"POS_Features.INTJ\",\n",
    "    \"POS_Features.X\",\n",
    "    \"tag_Features.RB\",\n",
    "    \"tag_Features.-RRB-\",\n",
    "    \"tag_Features.PRP$\",\n",
    "    \"tag_Features.JJ\",\n",
    "    \"tag_Features.TO\",\n",
    "    \"tag_Features.VBP\",\n",
    "    \"tag_Features.JJS\",\n",
    "    \"tag_Features.DT\",\n",
    "    \"tag_Features.''\",\n",
    "    \"tag_Features.UH\",\n",
    "    \"tag_Features.RBS\",\n",
    "    \"tag_Features.WRB\",\n",
    "    \"tag_Features..\",\n",
    "    \"tag_Features.HYPH\",\n",
    "    \"tag_Features.XX\",\n",
    "    \"tag_Features.``\",\n",
    "    \"tag_Features.SYM\",\n",
    "    \"tag_Features.VB\",\n",
    "    \"tag_Features.VBN\",\n",
    "    \"tag_Features.WP\",\n",
    "    \"tag_Features.CC\",\n",
    "    \"tag_Features.LS\",\n",
    "    \"tag_Features.POS\",\n",
    "    \"tag_Features.NN\",\n",
    "    \"tag_Features.,\",\n",
    "    \"tag_Features.NNPS\",\n",
    "    \"tag_Features.RP\",\n",
    "    \"tag_Features.:\",\n",
    "    \"tag_Features.$\",\n",
    "    \"tag_Features.PDT\",\n",
    "    \"tag_Features.VBZ\",\n",
    "    \"tag_Features.VBD\",\n",
    "    \"tag_Features.JJR\",\n",
    "    \"tag_Features.-LRB-\",\n",
    "    \"tag_Features.IN\",\n",
    "    \"tag_Features.RBR\",\n",
    "    \"tag_Features.WDT\",\n",
    "    \"tag_Features.EX\",\n",
    "    \"tag_Features.MD\",\n",
    "    \"tag_Features._SP\",\n",
    "    \"tag_Features.NNP\",\n",
    "    \"tag_Features.CD\",\n",
    "    \"tag_Features.VBG\",\n",
    "    \"tag_Features.NNS\",\n",
    "    \"tag_Features.PRP\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBMで用いるパラメータを指定\n",
    "params = {\n",
    "    \"objective\": \"regression\",\n",
    "    \"metric\": \"rmse\",\n",
    "    \"max_depth\": -1,\n",
    "    \"min_data_in_leaf\": 10,  # 1つの葉に入る最小のデータ数\n",
    "    \"num_leaves\": 24,  # 2**max_depthより少し小さめにすると過学習を防げる\n",
    "    \"learning_rate\": 0.01,  # 1回のiterationで学習を進める割合、大きいと学習が早く終わる。小さいと学習は長いが高精度になりやすい。\n",
    "    \"bagging_freq\": 5,  # 指定した回数ごとにbaggingを行う\n",
    "    \"feature_fraction\": 0.7,  # 1回のiterationで利用する特徴量(列方向)の割合\n",
    "    \"bagging_fraction\": 0.6,  # 1回のiterationで利用するデータ(行方向)の割合\n",
    "    \"verbose\": -1,  # 出力するログレベルの変更、-1はFatalなログのみを出力\n",
    "    \"seed\": SEED,  # ランダムシードの固定\n",
    "    \"lambda_l1\": 0.4,  # 正則化のためのパラメータ\n",
    "    \"lambda_l2\": 0.4,  # 正則化のためのパラメータ\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "oofs = []\n",
    "\n",
    "# Cross Validationによる学習の実施\n",
    "for fold in range(N_FOLD):\n",
    "    print(f\"Start fold {fold}\")\n",
    "\n",
    "    # foldごとにtrainとvalidに分ける\n",
    "    train_fold = train.filter(pl.col(\"fold\") != fold)\n",
    "    valid_fold = train.filter(pl.col(\"fold\") == fold)\n",
    "\n",
    "    # X(説明変数)とy(目的変数)に分ける\n",
    "    X_train = train_fold.select(USE_COL)\n",
    "    X_valid = valid_fold.select(USE_COL)\n",
    "    y_train = train_fold.select(TARGET_COL)\n",
    "    y_valid = valid_fold.select(TARGET_COL)\n",
    "\n",
    "    # LightGBMが認識可能な形にデータセットを変換\n",
    "    # polars.DataFrame から pandas.DataFrame への変更を行っている\n",
    "    lgb_train = lgb.Dataset(X_train.to_pandas(), y_train.to_pandas())\n",
    "    lgb_eval = lgb.Dataset(\n",
    "        X_valid.to_pandas(), y_valid.to_pandas(), reference=lgb_train\n",
    "    )\n",
    "\n",
    "    # モデルの学習\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        lgb_train,\n",
    "        num_boost_round=10000,  # 学習のiteration回数\n",
    "        valid_sets=[lgb_train, lgb_eval],\n",
    "        callbacks=[\n",
    "            early_stopping(\n",
    "                stopping_rounds=100\n",
    "            ),  # Early stopingの回数、binary_loglossが改善しないiterationが100回続いたら学習を止める\n",
    "            log_evaluation(100),  # 指定したiteration回数ごとにlogを出力する\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    with open(f\"{MODEL_OUTPUT_PATH}/model_{fold}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "    # モデルを保存\n",
    "    models.append([fold, model])\n",
    "\n",
    "    # valid データに対する推論\n",
    "    y_valid_pred = model.predict(X_valid.to_pandas())\n",
    "\n",
    "    # OOF に推論結果を保存\n",
    "    oof_per_fold = valid_fold.select(\"score\").with_columns(\n",
    "        pl.Series(y_valid_pred).alias(\"valid_pred\")\n",
    "    )\n",
    "    oofs.append(oof_per_fold)\n",
    "\n",
    "oof = pl.concat(oofs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特徴量重要度の計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 特徴量重要度を列にもつDataFrameを作成\n",
    "feature_importances = [\n",
    "    model.feature_importance(importance_type=\"gain\") for _, model in models\n",
    "]\n",
    "feature_importances_df = pd.DataFrame(feature_importances, columns=USE_COL)\n",
    "\n",
    "# 表示する順番を指定、特徴量重要度の平均が大きい順に並ぶよう計算\n",
    "order = feature_importances_df.mean().sort_values(ascending=False).index.tolist()\n",
    "\n",
    "# 表示\n",
    "# fold毎の特徴量重要度のばらつきを見るために、箱ひげ図を利用\n",
    "sns.boxplot(data=feature_importances_df, orient=\"h\", order=order)\n",
    "plt.savefig(f\"{MODEL_OUTPUT_PATH}/feature_importance.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CVの計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_score = cohen_kappa_score(\n",
    "    oof[\"score\"],\n",
    "    np.clip(oof[\"valid_pred\"], 1, 6).round(),\n",
    "    weights=\"quadratic\",\n",
    ")\n",
    "print(f\"OOF CV Score by round: {cv_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://qiita.com/kaggle_grandmaster-arai-san/items/d59b2fb7142ec7e270a5#optimizedrounder\n",
    "class OptimizedRounder:\n",
    "    def __init__(self):\n",
    "        self.coef_ = 0\n",
    "\n",
    "    def _kappa_loss(self, coef, X, y):\n",
    "        X_p = np.copy(X)\n",
    "        for i, pred in enumerate(X_p):\n",
    "            if pred < coef[0]:\n",
    "                X_p[i] = 1\n",
    "            elif pred >= coef[0] and pred < coef[1]:\n",
    "                X_p[i] = 2\n",
    "            elif pred >= coef[1] and pred < coef[2]:\n",
    "                X_p[i] = 3\n",
    "            elif pred >= coef[2] and pred < coef[3]:\n",
    "                X_p[i] = 4\n",
    "            elif pred >= coef[3] and pred < coef[4]:\n",
    "                X_p[i] = 5\n",
    "            else:\n",
    "                X_p[i] = 6\n",
    "\n",
    "        ll = cohen_kappa_score(y, X_p, weights=\"quadratic\")\n",
    "        return -ll\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        loss_partial = partial(self._kappa_loss, X=X, y=y)\n",
    "        initial_coef = [1.5, 2.5, 3.5, 4.5, 5.5]\n",
    "        self.coef_ = sp.optimize.minimize(\n",
    "            loss_partial, initial_coef, method=\"nelder-mead\"\n",
    "        )\n",
    "\n",
    "    def predict(self, X, coef):\n",
    "        X_p = np.copy(X)\n",
    "        for i, pred in enumerate(X_p):\n",
    "            if pred < coef[0]:\n",
    "                X_p[i] = 1\n",
    "            elif pred >= coef[0] and pred < coef[1]:\n",
    "                X_p[i] = 2\n",
    "            elif pred >= coef[1] and pred < coef[2]:\n",
    "                X_p[i] = 3\n",
    "            elif pred >= coef[2] and pred < coef[3]:\n",
    "                X_p[i] = 4\n",
    "            elif pred >= coef[3] and pred < coef[4]:\n",
    "                X_p[i] = 5\n",
    "            else:\n",
    "                X_p[i] = 6\n",
    "        return X_p\n",
    "\n",
    "    @property\n",
    "    def coefficients(self):\n",
    "        return self.coef_[\"x\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optR = OptimizedRounder()\n",
    "optR.fit(oof[\"valid_pred\"], oof[\"score\"])\n",
    "print(optR.coefficients)\n",
    "\n",
    "optimized_valid_pred = optR.predict(oof[\"valid_pred\"], optR.coefficients)\n",
    "np.save(f\"{MODEL_OUTPUT_PATH}/opt_thr.npy\", optR.coefficients)\n",
    "\n",
    "cv_score = cohen_kappa_score(oof[\"score\"], optimized_valid_pred, weights=\"quadratic\")\n",
    "\n",
    "print(f\"OOF CV Score by NelderMead: {cv_score}\")\n",
    "\n",
    "# output_textを保存\n",
    "with open(f\"{MODEL_OUTPUT_PATH}/cv_score.txt\", \"w\") as f:\n",
    "    f.write(str(cv_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 混同行列の計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(\n",
    "    oof[\"score\"],\n",
    "    optimized_valid_pred,\n",
    "    labels=[x for x in range(1, 7)],\n",
    ")\n",
    "\n",
    "draw_cm = ConfusionMatrixDisplay(\n",
    "    confusion_matrix=cm, display_labels=[x for x in range(1, 7)]\n",
    ")\n",
    "\n",
    "draw_cm.plot()\n",
    "plt.savefig(f\"{MODEL_OUTPUT_PATH}/confusion_matrix.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggleへのアップロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if UPLOAD_DATA_TO_KAGGLE:\n",
    "    import os\n",
    "    import json\n",
    "\n",
    "    from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "    def dataset_create_new(dataset_name: str, upload_dir: str):\n",
    "        # if \"_\" in dataset_name:\n",
    "        #     raise ValueError(\"datasetの名称に_の使用は禁止です\")\n",
    "        dataset_metadata = {}\n",
    "        dataset_metadata[\"id\"] = f\"sinchir0/{dataset_name}\"\n",
    "        dataset_metadata[\"licenses\"] = [{\"name\": \"CC0-1.0\"}]\n",
    "        dataset_metadata[\"title\"] = dataset_name\n",
    "        with open(os.path.join(upload_dir, \"dataset-metadata.json\"), \"w\") as f:\n",
    "            json.dump(dataset_metadata, f, indent=4)\n",
    "        api = KaggleApi()\n",
    "        api.authenticate()\n",
    "        api.dataset_create_new(folder=upload_dir, convert_to_csv=False, dir_mode=\"tar\")\n",
    "\n",
    "    print(f\"Create Dataset name:{DATASET_NAME}, output_dir:{MODEL_OUTPUT_PATH}\")\n",
    "    dataset_create_new(dataset_name=DATASET_NAME, upload_dir=MODEL_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
