{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 目的\n",
    "embedを諸々追加する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path setting\n",
    "EXP_NAME = \"e111-add-text-embed\"\n",
    "MODEL_NAME = \"lightgbm\"\n",
    "COMPETITION_NAME = \"automated_essay_scoring\"\n",
    "\n",
    "DATA_PATH = \"data\"\n",
    "DATASET_NAME = f\"{EXP_NAME}-{MODEL_NAME.split('/')[-1]}\"\n",
    "MODEL_OUTPUT_PATH = f\"trained_models/{EXP_NAME}\"\n",
    "\n",
    "N_FOLD = 3\n",
    "\n",
    "# experiment parameter\n",
    "DEBUG = False\n",
    "TRAINING = True\n",
    "UPLOAD_DATA_TO_S3 = False\n",
    "UPLOAD_DATA_TO_KAGGLE = False\n",
    "WANDB = True\n",
    "\n",
    "# model parameter\n",
    "SEED = 42\n",
    "EPOCH = 4\n",
    "LR = 2e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_dataset_name(dataset_name: str) -> None:\n",
    "    if len(dataset_name) < 6 or len(dataset_name) > 50:\n",
    "        raise Exception(\n",
    "            f\"データセットの文字列は6~50文字にしてください。現在{len(DATASET_NAME)}文字\"\n",
    "        )\n",
    "    if \"_\" in dataset_name:\n",
    "        raise Exception(\"datasetの名称に_の使用は禁止です\")\n",
    "\n",
    "\n",
    "validate_dataset_name(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.7\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/shinichiro.saito/automated_essay_scoring/automated_essay_scoring/exp\n",
      "Local Mac!\n",
      "../../data\n",
      "/Users/shinichiro.saito/automated_essay_scoring/automated_essay_scoring/exp\n",
      "Local Mac!\n",
      "../../trained_models/e111-add-text-embed\n"
     ]
    }
   ],
   "source": [
    "def resolve_path(base_path: str) -> str:\n",
    "    import os\n",
    "\n",
    "    cwd = os.getcwd()\n",
    "    print(cwd)\n",
    "    if cwd == f\"/notebooks\":\n",
    "        print(\"Jupyter Kernel By VSCode!\")\n",
    "        return f\"/notebooks/{COMPETITION_NAME}/{base_path}\"\n",
    "    elif cwd == f\"/notebooks/{COMPETITION_NAME}\":\n",
    "        print(\"nohup!\")\n",
    "        return base_path\n",
    "    elif cwd == f\"/notebooks/{COMPETITION_NAME}/{COMPETITION_NAME}/exp\":\n",
    "        print(\"Jupyter Lab!\")\n",
    "        return f\"../../{base_path}\"\n",
    "    elif cwd.startswith(\"/Users\"):\n",
    "        print(\"Local Mac!\")\n",
    "        return f\"../../{base_path}\"\n",
    "    else:\n",
    "        raise Exception(\"Unknown environment\")\n",
    "\n",
    "\n",
    "DATA_PATH = resolve_path(DATA_PATH)\n",
    "print(DATA_PATH)\n",
    "MODEL_OUTPUT_PATH = resolve_path(MODEL_OUTPUT_PATH)\n",
    "print(MODEL_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(MODEL_OUTPUT_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import torch\n",
    "import json\n",
    "import re\n",
    "import spacy\n",
    "import polars as pl\n",
    "import lightgbm as lgb\n",
    "from functools import partial\n",
    "from lightgbm import log_evaluation, early_stopping\n",
    "import wandb\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed the same seed to all\n",
    "def seed_everything(seed: int) -> None:\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(f\"{DATA_PATH}/.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(f\"{DATA_PATH}/essay_id_spelling_errors_cnt.json\", \"r\") as f:\n",
    "with open(f\"{DATA_PATH}/essay_id_spelling_errors_cnt_pyspell.json\", \"r\") as f:\n",
    "    essay_id_spelling_errors_cnt = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msinchir0\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/shinichiro.saito/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/shinichiro.saito/automated_essay_scoring/automated_essay_scoring/exp/wandb/run-20240702_073600-5l9n8kkd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sinchir0/automated_essay_scoring/runs/5l9n8kkd' target=\"_blank\">e111-add-text-embed</a></strong> to <a href='https://wandb.ai/sinchir0/automated_essay_scoring' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sinchir0/automated_essay_scoring' target=\"_blank\">https://wandb.ai/sinchir0/automated_essay_scoring</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sinchir0/automated_essay_scoring/runs/5l9n8kkd' target=\"_blank\">https://wandb.ai/sinchir0/automated_essay_scoring/runs/5l9n8kkd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'wandb'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if WANDB:\n",
    "    wandb.login(key=os.environ[\"WANDB_API_KEY\"])\n",
    "    wandb.init(project=\"automated_essay_scoring\", name=EXP_NAME)\n",
    "    REPORT_TO = \"wandb\"\n",
    "else:\n",
    "    REPORT_TO = \"none\"\n",
    "\n",
    "REPORT_TO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pl.read_csv(f\"{DATA_PATH}/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>essay_id</th><th>full_text</th><th>score</th></tr><tr><td>str</td><td>str</td><td>i64</td></tr></thead><tbody><tr><td>&quot;000d118&quot;</td><td>&quot;Many people have car where the…</td><td>3</td></tr><tr><td>&quot;000fe60&quot;</td><td>&quot;I am a scientist at NASA that …</td><td>3</td></tr><tr><td>&quot;001ab80&quot;</td><td>&quot;People always wish they had th…</td><td>4</td></tr><tr><td>&quot;001bdc0&quot;</td><td>&quot;We all heard about Venus, the …</td><td>4</td></tr><tr><td>&quot;002ba53&quot;</td><td>&quot;Dear, State Senator\n",
       "\n",
       "This is a…</td><td>3</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 3)\n",
       "┌──────────┬─────────────────────────────────┬───────┐\n",
       "│ essay_id ┆ full_text                       ┆ score │\n",
       "│ ---      ┆ ---                             ┆ ---   │\n",
       "│ str      ┆ str                             ┆ i64   │\n",
       "╞══════════╪═════════════════════════════════╪═══════╡\n",
       "│ 000d118  ┆ Many people have car where the… ┆ 3     │\n",
       "│ 000fe60  ┆ I am a scientist at NASA that … ┆ 3     │\n",
       "│ 001ab80  ┆ People always wish they had th… ┆ 4     │\n",
       "│ 001bdc0  ┆ We all heard about Venus, the … ┆ 4     │\n",
       "│ 002ba53  ┆ Dear, State Senator             ┆ 3     │\n",
       "│          ┆                                 ┆       │\n",
       "│          ┆ This is a…                      ┆       │\n",
       "└──────────┴─────────────────────────────────┴───────┘"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{DATA_PATH}/essay_id_fold_by_s_sl_g_p_only_train_dict.json\") as f:\n",
    "    essay_id_fold_only_train = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.with_columns(\n",
    "    pl.col(\"essay_id\")\n",
    "    .replace(essay_id_fold_only_train, return_dtype=pl.Int64)\n",
    "    .alias(\"fold\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_e053 = pl.read_csv(f\"{DATA_PATH}/e053-cv-w-oof/oof.csv\").rename(\n",
    "    {\"valid_pred\": \"e053_xsmall_oof\"}\n",
    ")\n",
    "train = train.join(oof_e053, on=\"essay_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_e054 = pl.read_csv(f\"{DATA_PATH}/e054-cv-w-oof/oof.csv\").rename(\n",
    "    {\"valid_pred\": \"e054_small_oof\"}\n",
    ")\n",
    "train = train.join(oof_e054, on=\"essay_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_e055 = pl.read_csv(f\"{DATA_PATH}/e055-cv-w-oof/oof.csv\").rename(\n",
    "    {\"valid_pred\": \"e055_base_oof\"}\n",
    ")\n",
    "train = train.join(oof_e055, on=\"essay_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_e056 = pl.read_csv(f\"{DATA_PATH}/e056-cv-w-oof/oof.csv\").rename(\n",
    "    {\"valid_pred\": \"e056_large_oof\"}\n",
    ")\n",
    "train = train.join(oof_e056, on=\"essay_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 12)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>essay_id</th><th>full_text</th><th>score</th><th>fold</th><th>e053_xsmall_oof</th><th>e054_small_oof</th><th>e055_base_oof</th><th>e056_large_oof</th><th>text_length</th><th>word_length</th><th>spelling_errors_cnt</th><th>rate_spelling_errors_per_word</th></tr><tr><td>str</td><td>str</td><td>i64</td><td>i64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>u32</td><td>i64</td><td>i64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;000d118&quot;</td><td>&quot;Many people have car where the…</td><td>3</td><td>0</td><td>2.59375</td><td>2.4804688</td><td>2.4941406</td><td>2.3769531</td><td>2677</td><td>498</td><td>22</td><td>0.044177</td></tr><tr><td>&quot;000fe60&quot;</td><td>&quot;I am a scientist at NASA that …</td><td>3</td><td>0</td><td>2.5761719</td><td>2.2910156</td><td>2.3007812</td><td>2.7050781</td><td>1669</td><td>332</td><td>5</td><td>0.01506</td></tr><tr><td>&quot;001ab80&quot;</td><td>&quot;People always wish they had th…</td><td>4</td><td>1</td><td>4.4882812</td><td>4.1523438</td><td>4.53125</td><td>4.4921875</td><td>3077</td><td>550</td><td>7</td><td>0.012727</td></tr><tr><td>&quot;001bdc0&quot;</td><td>&quot;We all heard about Venus, the …</td><td>4</td><td>0</td><td>3.4960938</td><td>3.3242188</td><td>3.0800781</td><td>3.2382812</td><td>2701</td><td>451</td><td>6</td><td>0.013304</td></tr><tr><td>&quot;002ba53&quot;</td><td>&quot;Dear, State Senator\n",
       "\n",
       "This is a…</td><td>3</td><td>2</td><td>2.8261719</td><td>2.9257812</td><td>3.0273438</td><td>2.7851562</td><td>2208</td><td>373</td><td>10</td><td>0.02681</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 12)\n",
       "┌──────────┬─────────────┬───────┬──────┬───┬─────────────┬─────────────┬─────────────┬────────────┐\n",
       "│ essay_id ┆ full_text   ┆ score ┆ fold ┆ … ┆ text_length ┆ word_length ┆ spelling_er ┆ rate_spell │\n",
       "│ ---      ┆ ---         ┆ ---   ┆ ---  ┆   ┆ ---         ┆ ---         ┆ rors_cnt    ┆ ing_errors │\n",
       "│ str      ┆ str         ┆ i64   ┆ i64  ┆   ┆ u32         ┆ i64         ┆ ---         ┆ _per_word  │\n",
       "│          ┆             ┆       ┆      ┆   ┆             ┆             ┆ i64         ┆ ---        │\n",
       "│          ┆             ┆       ┆      ┆   ┆             ┆             ┆             ┆ f64        │\n",
       "╞══════════╪═════════════╪═══════╪══════╪═══╪═════════════╪═════════════╪═════════════╪════════════╡\n",
       "│ 000d118  ┆ Many people ┆ 3     ┆ 0    ┆ … ┆ 2677        ┆ 498         ┆ 22          ┆ 0.044177   │\n",
       "│          ┆ have car    ┆       ┆      ┆   ┆             ┆             ┆             ┆            │\n",
       "│          ┆ where the…  ┆       ┆      ┆   ┆             ┆             ┆             ┆            │\n",
       "│ 000fe60  ┆ I am a      ┆ 3     ┆ 0    ┆ … ┆ 1669        ┆ 332         ┆ 5           ┆ 0.01506    │\n",
       "│          ┆ scientist   ┆       ┆      ┆   ┆             ┆             ┆             ┆            │\n",
       "│          ┆ at NASA     ┆       ┆      ┆   ┆             ┆             ┆             ┆            │\n",
       "│          ┆ that …      ┆       ┆      ┆   ┆             ┆             ┆             ┆            │\n",
       "│ 001ab80  ┆ People      ┆ 4     ┆ 1    ┆ … ┆ 3077        ┆ 550         ┆ 7           ┆ 0.012727   │\n",
       "│          ┆ always wish ┆       ┆      ┆   ┆             ┆             ┆             ┆            │\n",
       "│          ┆ they had    ┆       ┆      ┆   ┆             ┆             ┆             ┆            │\n",
       "│          ┆ th…         ┆       ┆      ┆   ┆             ┆             ┆             ┆            │\n",
       "│ 001bdc0  ┆ We all      ┆ 4     ┆ 0    ┆ … ┆ 2701        ┆ 451         ┆ 6           ┆ 0.013304   │\n",
       "│          ┆ heard about ┆       ┆      ┆   ┆             ┆             ┆             ┆            │\n",
       "│          ┆ Venus, the  ┆       ┆      ┆   ┆             ┆             ┆             ┆            │\n",
       "│          ┆ …           ┆       ┆      ┆   ┆             ┆             ┆             ┆            │\n",
       "│ 002ba53  ┆ Dear, State ┆ 3     ┆ 2    ┆ … ┆ 2208        ┆ 373         ┆ 10          ┆ 0.02681    │\n",
       "│          ┆ Senator     ┆       ┆      ┆   ┆             ┆             ┆             ┆            │\n",
       "│          ┆             ┆       ┆      ┆   ┆             ┆             ┆             ┆            │\n",
       "│          ┆ This is a…  ┆       ┆      ┆   ┆             ┆             ┆             ┆            │\n",
       "└──────────┴─────────────┴───────┴──────┴───┴─────────────┴─────────────┴─────────────┴────────────┘"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_text_length() -> pl.Expr:\n",
    "    return pl.col(\"full_text\").str.len_chars().alias(\"text_length\")\n",
    "\n",
    "\n",
    "def get_word_length() -> pl.Expr:\n",
    "    return (\n",
    "        pl.col(\"full_text\")\n",
    "        .map_elements(lambda x: len(x.split()), return_dtype=pl.Int64)\n",
    "        .alias(\"word_length\")\n",
    "    )\n",
    "\n",
    "\n",
    "def count_spelling_errors() -> pl.Expr:\n",
    "    return (\n",
    "        pl.col(\"essay_id\")\n",
    "        .replace(essay_id_spelling_errors_cnt, return_dtype=pl.Int64)\n",
    "        .alias(\"spelling_errors_cnt\")\n",
    "    )\n",
    "\n",
    "\n",
    "def rate_spelling_errors_per_word() -> pl.Expr:\n",
    "    return (pl.col(\"spelling_errors_cnt\") / pl.col(\"word_length\")).alias(\n",
    "        \"rate_spelling_errors_per_word\"\n",
    "    )\n",
    "\n",
    "\n",
    "train = train.with_columns(\n",
    "    get_text_length(), get_word_length(), count_spelling_errors()\n",
    ").with_columns(rate_spelling_errors_per_word())\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def textstat_features(text):\n",
    "    features = {}\n",
    "    features[\"flesch_reading_ease\"] = textstat.flesch_reading_ease(text)\n",
    "    features[\"flesch_kincaid_grade\"] = textstat.flesch_kincaid_grade(text)\n",
    "    features[\"smog_index\"] = textstat.smog_index(text)\n",
    "    features[\"coleman_liau_index\"] = textstat.coleman_liau_index(text)\n",
    "    features[\"automated_readability_index\"] = textstat.automated_readability_index(text)\n",
    "    features[\"dale_chall_readability_score\"] = textstat.dale_chall_readability_score(\n",
    "        text\n",
    "    )\n",
    "    features[\"difficult_words\"] = textstat.difficult_words(text)\n",
    "    features[\"linsear_write_formula\"] = textstat.linsear_write_formula(text)\n",
    "    features[\"gunning_fog\"] = textstat.gunning_fog(text)\n",
    "    features[\"text_standard\"] = textstat.text_standard(text, float_output=True)\n",
    "    features[\"spache_readability\"] = textstat.spache_readability(text)\n",
    "    features[\"mcalpine_eflaw\"] = textstat.mcalpine_eflaw(text)\n",
    "    features[\"reading_time\"] = textstat.reading_time(text)\n",
    "    features[\"syllable_count\"] = textstat.syllable_count(text)\n",
    "    features[\"lexicon_count\"] = textstat.lexicon_count(text)\n",
    "    features[\"monosyllabcount\"] = textstat.monosyllabcount(text)\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "train_text_stat_df = pl.DataFrame(\n",
    "    train[\"full_text\"].to_pandas().apply(textstat_features).tolist()\n",
    ")\n",
    "\n",
    "train = pl.concat([train, train_text_stat_df], how=\"horizontal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "# nltk.download()\n",
    "\n",
    "\n",
    "def extract_linguistic_features(text):\n",
    "\n",
    "    doc = nlp(text)\n",
    "    features = {}\n",
    "\n",
    "    # NER Features\n",
    "    entity_counts = {\n",
    "        \"GPE\": 0,\n",
    "        \"PERCENT\": 0,\n",
    "        \"NORP\": 0,\n",
    "        \"ORG\": 0,\n",
    "        \"CARDINAL\": 0,\n",
    "        \"MONEY\": 0,\n",
    "        \"DATE\": 0,\n",
    "        \"LOC\": 0,\n",
    "        \"PERSON\": 0,\n",
    "        \"QUANTITY\": 0,\n",
    "        \"EVENT\": 0,\n",
    "        \"ORDINAL\": 0,\n",
    "        \"WORK_OF_ART\": 0,\n",
    "        \"LAW\": 0,\n",
    "        \"PRODUCT\": 0,\n",
    "        \"TIME\": 0,\n",
    "        \"FAC\": 0,\n",
    "        \"LANGUAGE\": 0,\n",
    "    }\n",
    "    for entity in doc.ents:\n",
    "        if entity.label_ in entity_counts:\n",
    "            entity_counts[entity.label_] += 1\n",
    "    features[\"NER_Features\"] = entity_counts\n",
    "\n",
    "    # POS Features\n",
    "    pos_counts = {\n",
    "        \"ADJ\": 0,\n",
    "        \"NOUN\": 0,\n",
    "        \"VERB\": 0,\n",
    "        \"SCONJ\": 0,\n",
    "        \"PRON\": 0,\n",
    "        \"PUNCT\": 0,\n",
    "        \"DET\": 0,\n",
    "        \"AUX\": 0,\n",
    "        \"PART\": 0,\n",
    "        \"ADP\": 0,\n",
    "        \"SPACE\": 0,\n",
    "        \"CCONJ\": 0,\n",
    "        \"PROPN\": 0,\n",
    "        \"NUM\": 0,\n",
    "        \"ADV\": 0,\n",
    "        \"SYM\": 0,\n",
    "        \"INTJ\": 0,\n",
    "        \"X\": 0,\n",
    "    }\n",
    "    for token in doc:\n",
    "        if token.pos_ in pos_counts:\n",
    "            pos_counts[token.pos_] += 1\n",
    "    features[\"POS_Features\"] = pos_counts\n",
    "\n",
    "    # tag Features\n",
    "    tags = {\n",
    "        \"RB\": 0,\n",
    "        \"-RRB-\": 0,\n",
    "        \"PRP$\": 0,\n",
    "        \"JJ\": 0,\n",
    "        \"TO\": 0,\n",
    "        \"VBP\": 0,\n",
    "        \"JJS\": 0,\n",
    "        \"DT\": 0,\n",
    "        \"''\": 0,\n",
    "        \"UH\": 0,\n",
    "        \"RBS\": 0,\n",
    "        \"WRB\": 0,\n",
    "        \".\": 0,\n",
    "        \"HYPH\": 0,\n",
    "        \"XX\": 0,\n",
    "        \"``\": 0,\n",
    "        \"SYM\": 0,\n",
    "        \"VB\": 0,\n",
    "        \"VBN\": 0,\n",
    "        \"WP\": 0,\n",
    "        \"CC\": 0,\n",
    "        \"LS\": 0,\n",
    "        \"POS\": 0,\n",
    "        \"NN\": 0,\n",
    "        \",\": 0,\n",
    "        \"NNPS\": 0,\n",
    "        \"RP\": 0,\n",
    "        \":\": 0,\n",
    "        \"$\": 0,\n",
    "        \"PDT\": 0,\n",
    "        \"VBZ\": 0,\n",
    "        \"VBD\": 0,\n",
    "        \"JJR\": 0,\n",
    "        \"-LRB-\": 0,\n",
    "        \"IN\": 0,\n",
    "        \"RBR\": 0,\n",
    "        \"WDT\": 0,\n",
    "        \"EX\": 0,\n",
    "        \"MD\": 0,\n",
    "        \"_SP\": 0,\n",
    "        \"NNP\": 0,\n",
    "        \"CD\": 0,\n",
    "        \"VBG\": 0,\n",
    "        \"NNS\": 0,\n",
    "        \"PRP\": 0,\n",
    "    }\n",
    "\n",
    "    for token in doc:\n",
    "        if token.tag_ in tags:\n",
    "            tags[token.tag_] += 1\n",
    "    features[\"tag_Features\"] = tags\n",
    "\n",
    "    # tense features\n",
    "    tenses = [i.morph.get(\"Tense\") for i in doc]\n",
    "    tenses = [i[0] for i in tenses if i]\n",
    "    tense_counts = Counter(tenses)\n",
    "    features[\"past_tense_ratio\"] = tense_counts.get(\"Past\", 0) / (\n",
    "        tense_counts.get(\"Pres\", 0) + tense_counts.get(\"Past\", 0) + 1e-5\n",
    "    )\n",
    "    features[\"present_tense_ratio\"] = tense_counts.get(\"Pres\", 0) / (\n",
    "        tense_counts.get(\"Pres\", 0) + tense_counts.get(\"Past\", 0) + 1e-5\n",
    "    )\n",
    "\n",
    "    # len features\n",
    "\n",
    "    features[\"word_count\"] = len(doc)\n",
    "    features[\"sentence_count\"] = len([sentence for sentence in doc.sents])\n",
    "    features[\"words_per_sentence\"] = features[\"word_count\"] / features[\"sentence_count\"]\n",
    "    features[\"std_words_per_sentence\"] = np.std(\n",
    "        [len(sentence) for sentence in doc.sents]\n",
    "    )\n",
    "\n",
    "    features[\"unique_words\"] = len(set([token.text for token in doc]))\n",
    "    features[\"lexical_diversity\"] = features[\"unique_words\"] / features[\"word_count\"]\n",
    "\n",
    "    paragraph = text.split(\"\\n\\n\")\n",
    "\n",
    "    features[\"paragraph_count\"] = len(paragraph)\n",
    "\n",
    "    features[\"avg_chars_by_paragraph\"] = np.mean(\n",
    "        [len(paragraph) for paragraph in paragraph]\n",
    "    )\n",
    "    features[\"avg_words_by_paragraph\"] = np.mean(\n",
    "        [len(nltk.word_tokenize(paragraph)) for paragraph in paragraph]\n",
    "    )\n",
    "    features[\"avg_sentences_by_paragraph\"] = np.mean(\n",
    "        [len(nltk.sent_tokenize(paragraph)) for paragraph in paragraph]\n",
    "    )\n",
    "\n",
    "    # sentiment features\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "    compound_scores, negative_scores, positive_scores, neutral_scores = [], [], [], []\n",
    "    for sentence in sentences:\n",
    "        scores = analyzer.polarity_scores(sentence)\n",
    "        compound_scores.append(scores[\"compound\"])\n",
    "        negative_scores.append(scores[\"neg\"])\n",
    "        positive_scores.append(scores[\"pos\"])\n",
    "        neutral_scores.append(scores[\"neu\"])\n",
    "\n",
    "    features[\"mean_compound\"] = np.mean(compound_scores)\n",
    "    features[\"mean_negative\"] = np.mean(negative_scores)\n",
    "    features[\"mean_positive\"] = np.mean(positive_scores)\n",
    "    features[\"mean_neutral\"] = np.mean(neutral_scores)\n",
    "\n",
    "    features[\"std_compound\"] = np.std(compound_scores)\n",
    "    features[\"std_negative\"] = np.std(negative_scores)\n",
    "    features[\"std_positive\"] = np.std(positive_scores)\n",
    "    features[\"std_neutral\"] = np.std(neutral_scores)\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "# train_linguistic = pl.DataFrame(\n",
    "#     pd.json_normalize(\n",
    "#         train.to_pandas()[\"full_text\"].progress_apply(extract_linguistic_features)\n",
    "#     )\n",
    "# )\n",
    "# train_linguistic.write_csv(f\"{DATA_PATH}/train_linguistic.csv\")\n",
    "\n",
    "train_linguistic = pl.read_csv(f\"{DATA_PATH}/e106/train_linguistic.csv\")\n",
    "\n",
    "# LightGBMように変数名を修正\n",
    "train_linguistic = train_linguistic.rename(\n",
    "    {\n",
    "        \"tag_Features.,\": \"tag_Features.comma\",\n",
    "        \"tag_Features.PRP$\": \"tag_Features.PRP_doll\",\n",
    "        \"tag_Features.PRP$\": \"tag_Features.PRP_doll\",\n",
    "        \"tag_Features.''\": \"tag_Features.sinqle_quote\",\n",
    "        \"tag_Features..\": \"tag_Features.dot\",\n",
    "        \"tag_Features.``\": \"tag_Features.quotation\",\n",
    "        \"tag_Features.:\": \"tag_Features.semicoron\",\n",
    "        \"tag_Features.$\": \"tag_Features.doll\",\n",
    "    }\n",
    ")\n",
    "\n",
    "train = pl.concat([train, train_linguistic], how=\"horizontal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_FEEDBACK_FEATURE = True\n",
    "if not LOAD_FEEDBACK_FEATURE:\n",
    "    feedback_df = pd.read_csv(f\"{DATA_PATH}/e106/feedback-data/feedback_data.csv\")\n",
    "\n",
    "    feed_embeds = []\n",
    "\n",
    "    merged_embeds = []\n",
    "\n",
    "    test_embeds = []\n",
    "\n",
    "    for i in range(5):\n",
    "        model_path = f\"{DATA_PATH}/e106/sent-debsmall/deberta_small_trained/temp_fold{i}_checkpoints\"\n",
    "        word_embedding_model = models.Transformer(model_path, max_seq_length=1024)\n",
    "        pooling_model = models.Pooling(\n",
    "            word_embedding_model.get_word_embedding_dimension()\n",
    "        )\n",
    "        model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "\n",
    "        model.half()\n",
    "        model = model.to(\"cuda\")\n",
    "\n",
    "        feed_custom_embeddings_train = model.encode(\n",
    "            feedback_df.loc[:, \"full_text\"].values,\n",
    "            device=\"cuda\",\n",
    "            show_progress_bar=True,\n",
    "            normalize_embeddings=True,\n",
    "        )\n",
    "\n",
    "        feed_embeds.append(feed_custom_embeddings_train)\n",
    "\n",
    "        merged_custom_embeddings = model.encode(\n",
    "            train.to_pandas().loc[:, \"full_text\"].values,\n",
    "            device=\"cuda\",\n",
    "            show_progress_bar=True,\n",
    "            normalize_embeddings=True,\n",
    "        )\n",
    "\n",
    "        merged_embeds.append(merged_custom_embeddings)\n",
    "\n",
    "    #     test_custom_embeddings = model.encode(\n",
    "    #         test.loc[:, \"full_text\"].values,\n",
    "    #         device=\"cuda\",\n",
    "    #         show_progress_bar=True,\n",
    "    #         normalize_embeddings=True,\n",
    "    #     )\n",
    "\n",
    "    #     test_embeds.append(test_custom_embeddings)\n",
    "\n",
    "    feed_embeds = np.mean(feed_embeds, axis=0)\n",
    "    merged_embeds = np.mean(merged_embeds, axis=0)\n",
    "    # test_embeds = np.mean(test_embeds, axis=0)\n",
    "\n",
    "    from sklearn.linear_model import Ridge\n",
    "    from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "    targets = [\n",
    "        \"cohesion\",\n",
    "        \"syntax\",\n",
    "        \"vocabulary\",\n",
    "        \"phraseology\",\n",
    "        \"grammar\",\n",
    "        \"conventions\",\n",
    "    ]\n",
    "\n",
    "    ridge = Ridge(alpha=1.0)\n",
    "\n",
    "    multioutputregressor = MultiOutputRegressor(ridge)\n",
    "    multioutputregressor.fit(feed_embeds, feedback_df.loc[:, targets])\n",
    "\n",
    "    with open(f\"{MODEL_OUTPUT_PATH}/fb_ridge.pkl\", \"wb\") as file:\n",
    "        pickle.dump(multioutputregressor, file)\n",
    "\n",
    "    feedback_predictions = multioutputregressor.predict(merged_embeds)\n",
    "\n",
    "    feedback_predictions_df = pd.DataFrame(feedback_predictions, columns=targets)\n",
    "\n",
    "    # test_feedback_predictions = multioutputregressor.predict(test_embeds)\n",
    "    # test_feedback_predictions_df = pd.DataFrame(test_feedback_predictions, columns=targets)\n",
    "\n",
    "    feedback_predictions_df.head()\n",
    "\n",
    "    feedback_predictions_df.to_csv(\n",
    "        f\"{DATA_PATH}/e106/feedback-data/feedback_predictions.csv\"\n",
    "    )\n",
    "else:\n",
    "    feedback_predictions_df = pd.read_csv(\n",
    "        f\"{DATA_PATH}/e106/feedback-data/feedback_predictions.csv\", index_col=0\n",
    "    )\n",
    "\n",
    "train = pl.concat([train, pl.from_pandas(feedback_predictions_df)], how=\"horizontal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embedd = np.load(f\"{DATA_PATH}/RAPIDS_SVR_Starter/all_train_embeds.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embedd_df = pl.DataFrame(\n",
    "    train_embedd, schema=[f\"emb_{i}\" for i in range(train_embedd.shape[1])]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17307, 5376)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_embedd_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pl.concat([train, train_embedd_df], how=\"horizontal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_COL = \"score\"\n",
    "USE_COL = [\n",
    "    \"e053_xsmall_oof\",\n",
    "    \"e054_small_oof\",\n",
    "    \"e055_base_oof\",\n",
    "    \"e056_large_oof\",\n",
    "    \"text_length\",\n",
    "    \"word_length\",\n",
    "    \"spelling_errors_cnt\",\n",
    "    \"rate_spelling_errors_per_word\",\n",
    "]\n",
    "\n",
    "USE_COL += [\n",
    "    \"flesch_reading_ease\",\n",
    "    \"flesch_kincaid_grade\",\n",
    "    \"smog_index\",\n",
    "    \"coleman_liau_index\",\n",
    "    \"automated_readability_index\",\n",
    "    \"dale_chall_readability_score\",\n",
    "    \"difficult_words\",\n",
    "    \"linsear_write_formula\",\n",
    "    \"gunning_fog\",\n",
    "    \"text_standard\",\n",
    "    \"spache_readability\",\n",
    "    \"mcalpine_eflaw\",\n",
    "    \"reading_time\",\n",
    "    \"syllable_count\",\n",
    "    \"lexicon_count\",\n",
    "    \"monosyllabcount\",\n",
    "]\n",
    "\n",
    "USE_COL += [\n",
    "    \"past_tense_ratio\",\n",
    "    \"present_tense_ratio\",\n",
    "    \"word_count\",\n",
    "    \"sentence_count\",\n",
    "    \"words_per_sentence\",\n",
    "    \"std_words_per_sentence\",\n",
    "    \"unique_words\",\n",
    "    \"lexical_diversity\",\n",
    "    \"paragraph_count\",\n",
    "    \"avg_chars_by_paragraph\",\n",
    "    \"avg_words_by_paragraph\",\n",
    "    \"avg_sentences_by_paragraph\",\n",
    "    \"mean_compound\",\n",
    "    \"mean_negative\",\n",
    "    \"mean_positive\",\n",
    "    \"mean_neutral\",\n",
    "    \"std_compound\",\n",
    "    \"std_negative\",\n",
    "    \"std_positive\",\n",
    "    \"std_neutral\",\n",
    "    \"NER_Features.GPE\",\n",
    "    \"NER_Features.PERCENT\",\n",
    "    \"NER_Features.NORP\",\n",
    "    \"NER_Features.ORG\",\n",
    "    \"NER_Features.CARDINAL\",\n",
    "    \"NER_Features.MONEY\",\n",
    "    \"NER_Features.DATE\",\n",
    "    \"NER_Features.LOC\",\n",
    "    \"NER_Features.PERSON\",\n",
    "    \"NER_Features.QUANTITY\",\n",
    "    \"NER_Features.EVENT\",\n",
    "    \"NER_Features.ORDINAL\",\n",
    "    \"NER_Features.WORK_OF_ART\",\n",
    "    \"NER_Features.LAW\",\n",
    "    \"NER_Features.PRODUCT\",\n",
    "    \"NER_Features.TIME\",\n",
    "    \"NER_Features.FAC\",\n",
    "    \"NER_Features.LANGUAGE\",\n",
    "    \"POS_Features.ADJ\",\n",
    "    \"POS_Features.NOUN\",\n",
    "    \"POS_Features.VERB\",\n",
    "    \"POS_Features.SCONJ\",\n",
    "    \"POS_Features.PRON\",\n",
    "    \"POS_Features.PUNCT\",\n",
    "    \"POS_Features.DET\",\n",
    "    \"POS_Features.AUX\",\n",
    "    \"POS_Features.PART\",\n",
    "    \"POS_Features.ADP\",\n",
    "    \"POS_Features.SPACE\",\n",
    "    \"POS_Features.CCONJ\",\n",
    "    \"POS_Features.PROPN\",\n",
    "    \"POS_Features.NUM\",\n",
    "    \"POS_Features.ADV\",\n",
    "    \"POS_Features.SYM\",\n",
    "    \"POS_Features.INTJ\",\n",
    "    \"POS_Features.X\",\n",
    "    \"tag_Features.RB\",\n",
    "    \"tag_Features.-RRB-\",\n",
    "    \"tag_Features.PRP_doll\",\n",
    "    \"tag_Features.JJ\",\n",
    "    \"tag_Features.TO\",\n",
    "    \"tag_Features.VBP\",\n",
    "    \"tag_Features.JJS\",\n",
    "    \"tag_Features.DT\",\n",
    "    \"tag_Features.sinqle_quote\",\n",
    "    \"tag_Features.UH\",\n",
    "    \"tag_Features.RBS\",\n",
    "    \"tag_Features.WRB\",\n",
    "    \"tag_Features.dot\",\n",
    "    \"tag_Features.HYPH\",\n",
    "    \"tag_Features.XX\",\n",
    "    \"tag_Features.quotation\",\n",
    "    \"tag_Features.SYM\",\n",
    "    \"tag_Features.VB\",\n",
    "    \"tag_Features.VBN\",\n",
    "    \"tag_Features.WP\",\n",
    "    \"tag_Features.CC\",\n",
    "    \"tag_Features.LS\",\n",
    "    \"tag_Features.POS\",\n",
    "    \"tag_Features.NN\",\n",
    "    \"tag_Features.comma\",\n",
    "    \"tag_Features.NNPS\",\n",
    "    \"tag_Features.RP\",\n",
    "    \"tag_Features.semicoron\",\n",
    "    \"tag_Features.doll\",\n",
    "    \"tag_Features.PDT\",\n",
    "    \"tag_Features.VBZ\",\n",
    "    \"tag_Features.VBD\",\n",
    "    \"tag_Features.JJR\",\n",
    "    \"tag_Features.-LRB-\",\n",
    "    \"tag_Features.IN\",\n",
    "    \"tag_Features.RBR\",\n",
    "    \"tag_Features.WDT\",\n",
    "    \"tag_Features.EX\",\n",
    "    \"tag_Features.MD\",\n",
    "    \"tag_Features._SP\",\n",
    "    \"tag_Features.NNP\",\n",
    "    \"tag_Features.CD\",\n",
    "    \"tag_Features.VBG\",\n",
    "    \"tag_Features.NNS\",\n",
    "    \"tag_Features.PRP\",\n",
    "]\n",
    "\n",
    "USE_COL += [f\"emb_{i}\" for i in range(train_embedd.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBMで用いるパラメータを指定\n",
    "params = {\n",
    "    \"objective\": \"regression\",\n",
    "    \"metric\": \"rmse\",\n",
    "    \"max_depth\": -1,\n",
    "    \"min_data_in_leaf\": 10,  # 1つの葉に入る最小のデータ数\n",
    "    \"num_leaves\": 24,  # 2**max_depthより少し小さめにすると過学習を防げる\n",
    "    \"learning_rate\": 0.01,  # 1回のiterationで学習を進める割合、大きいと学習が早く終わる。小さいと学習は長いが高精度になりやすい。\n",
    "    \"bagging_freq\": 5,  # 指定した回数ごとにbaggingを行う\n",
    "    \"feature_fraction\": 0.7,  # 1回のiterationで利用する特徴量(列方向)の割合\n",
    "    \"bagging_fraction\": 0.6,  # 1回のiterationで利用するデータ(行方向)の割合\n",
    "    \"verbose\": -1,  # 出力するログレベルの変更、-1はFatalなログのみを出力\n",
    "    \"seed\": SEED,  # ランダムシードの固定\n",
    "    \"lambda_l1\": 0.4,  # 正則化のためのパラメータ\n",
    "    \"lambda_l2\": 0.4,  # 正則化のためのパラメータ\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start fold 0\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.615222\tvalid_1's rmse: 0.732638\n",
      "[200]\ttraining's rmse: 0.504494\tvalid_1's rmse: 0.696135\n",
      "Early stopping, best iteration is:\n",
      "[177]\ttraining's rmse: 0.519815\tvalid_1's rmse: 0.694272\n",
      "Start fold 1\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.659637\tvalid_1's rmse: 0.614149\n",
      "[200]\ttraining's rmse: 0.543065\tvalid_1's rmse: 0.55915\n",
      "[300]\ttraining's rmse: 0.499643\tvalid_1's rmse: 0.562859\n",
      "Early stopping, best iteration is:\n",
      "[226]\ttraining's rmse: 0.528455\tvalid_1's rmse: 0.557819\n",
      "Start fold 2\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.63849\tvalid_1's rmse: 0.708245\n",
      "[200]\ttraining's rmse: 0.535133\tvalid_1's rmse: 0.61742\n",
      "[300]\ttraining's rmse: 0.495924\tvalid_1's rmse: 0.598609\n",
      "[400]\ttraining's rmse: 0.471166\tvalid_1's rmse: 0.595328\n",
      "[500]\ttraining's rmse: 0.451533\tvalid_1's rmse: 0.596204\n",
      "Early stopping, best iteration is:\n",
      "[446]\ttraining's rmse: 0.46177\tvalid_1's rmse: 0.594646\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "oofs = []\n",
    "\n",
    "# Cross Validationによる学習の実施\n",
    "for fold in range(N_FOLD):\n",
    "    print(f\"Start fold {fold}\")\n",
    "\n",
    "    # foldごとにtrainとvalidに分ける\n",
    "    train_fold = train.filter(pl.col(\"fold\") != fold)\n",
    "    valid_fold = train.filter(pl.col(\"fold\") == fold)\n",
    "\n",
    "    # X(説明変数)とy(目的変数)に分ける\n",
    "    X_train = train_fold.select(USE_COL)\n",
    "    X_valid = valid_fold.select(USE_COL)\n",
    "    y_train = train_fold.select(TARGET_COL)\n",
    "    y_valid = valid_fold.select(TARGET_COL)\n",
    "\n",
    "    # LightGBMが認識可能な形にデータセットを変換\n",
    "    # polars.DataFrame から pandas.DataFrame への変更を行っている\n",
    "    lgb_train = lgb.Dataset(X_train.to_pandas(), y_train.to_pandas())\n",
    "    lgb_eval = lgb.Dataset(\n",
    "        X_valid.to_pandas(), y_valid.to_pandas(), reference=lgb_train\n",
    "    )\n",
    "\n",
    "    # モデルの学習\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        lgb_train,\n",
    "        num_boost_round=10000,  # 学習のiteration回数\n",
    "        valid_sets=[lgb_train, lgb_eval],\n",
    "        callbacks=[\n",
    "            early_stopping(\n",
    "                stopping_rounds=100\n",
    "            ),  # Early stopingの回数、binary_loglossが改善しないiterationが100回続いたら学習を止める\n",
    "            log_evaluation(100),  # 指定したiteration回数ごとにlogを出力する\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    with open(f\"{MODEL_OUTPUT_PATH}/model_{fold}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "    # モデルを保存\n",
    "    models.append([fold, model])\n",
    "\n",
    "    # valid データに対する推論\n",
    "    y_valid_pred = model.predict(X_valid.to_pandas())\n",
    "\n",
    "    # OOF に推論結果を保存\n",
    "    oof_per_fold = valid_fold.select(\"score\").with_columns(\n",
    "        pl.Series(y_valid_pred).alias(\"valid_pred\")\n",
    "    )\n",
    "    oofs.append(oof_per_fold)\n",
    "\n",
    "oof = pl.concat(oofs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特徴量重要度の計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function flush_figures at 0x36c53b9c0> (for post_execute), with arguments args (),kwargs {}:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/automated_essay_scoring/.venv/lib/python3.11/site-packages/matplotlib_inline/backend_inline.py:126\u001b[0m, in \u001b[0;36mflush_figures\u001b[0;34m()\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m InlineBackend\u001b[38;5;241m.\u001b[39minstance()\u001b[38;5;241m.\u001b[39mclose_figures:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;66;03m# ignore the tracking, just draw and close all figures\u001b[39;00m\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 126\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    128\u001b[0m         \u001b[38;5;66;03m# safely show traceback if in IPython, else raise\u001b[39;00m\n\u001b[1;32m    129\u001b[0m         ip \u001b[38;5;241m=\u001b[39m get_ipython()\n",
      "File \u001b[0;32m~/automated_essay_scoring/.venv/lib/python3.11/site-packages/matplotlib_inline/backend_inline.py:90\u001b[0m, in \u001b[0;36mshow\u001b[0;34m(close, block)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m figure_manager \u001b[38;5;129;01min\u001b[39;00m Gcf\u001b[38;5;241m.\u001b[39mget_all_fig_managers():\n\u001b[0;32m---> 90\u001b[0m         \u001b[43mdisplay\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfigure_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fetch_figure_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfigure_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     95\u001b[0m     show\u001b[38;5;241m.\u001b[39m_to_draw \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/automated_essay_scoring/.venv/lib/python3.11/site-packages/IPython/core/display_functions.py:298\u001b[0m, in \u001b[0;36mdisplay\u001b[0;34m(include, exclude, metadata, transient, display_id, raw, clear, *objs, **kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m     publish_display_data(data\u001b[38;5;241m=\u001b[39mobj, metadata\u001b[38;5;241m=\u001b[39mmetadata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 298\u001b[0m     format_dict, md_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m format_dict:\n\u001b[1;32m    300\u001b[0m         \u001b[38;5;66;03m# nothing to display (e.g. _ipython_display_ took over)\u001b[39;00m\n\u001b[1;32m    301\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/automated_essay_scoring/.venv/lib/python3.11/site-packages/IPython/core/formatters.py:182\u001b[0m, in \u001b[0;36mDisplayFormatter.format\u001b[0;34m(self, obj, include, exclude)\u001b[0m\n\u001b[1;32m    180\u001b[0m md \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 182\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# FIXME: log the exception\u001b[39;00m\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/automated_essay_scoring/.venv/lib/python3.11/site-packages/decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[1;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[0;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcaller\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mextras\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/automated_essay_scoring/.venv/lib/python3.11/site-packages/IPython/core/formatters.py:226\u001b[0m, in \u001b[0;36mcatch_format_error\u001b[0;34m(method, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"show traceback on failed format call\"\"\"\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 226\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# don't warn on NotImplementedErrors\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_return(\u001b[38;5;28;01mNone\u001b[39;00m, args[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/automated_essay_scoring/.venv/lib/python3.11/site-packages/IPython/core/formatters.py:343\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprinter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# Finally look for special method names\u001b[39;00m\n\u001b[1;32m    345\u001b[0m method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n",
      "File \u001b[0;32m~/automated_essay_scoring/.venv/lib/python3.11/site-packages/IPython/core/pylabtools.py:170\u001b[0m, in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, base64, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend_bases\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FigureCanvasBase\n\u001b[1;32m    168\u001b[0m     FigureCanvasBase(fig)\n\u001b[0;32m--> 170\u001b[0m \u001b[43mfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint_figure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbytes_io\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m data \u001b[38;5;241m=\u001b[39m bytes_io\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fmt \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msvg\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/automated_essay_scoring/.venv/lib/python3.11/site-packages/matplotlib/backend_bases.py:2164\u001b[0m, in \u001b[0;36mFigureCanvasBase.print_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2161\u001b[0m     \u001b[38;5;66;03m# we do this instead of `self.figure.draw_without_rendering`\u001b[39;00m\n\u001b[1;32m   2162\u001b[0m     \u001b[38;5;66;03m# so that we can inject the orientation\u001b[39;00m\n\u001b[1;32m   2163\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(renderer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_draw_disabled\u001b[39m\u001b[38;5;124m\"\u001b[39m, nullcontext)():\n\u001b[0;32m-> 2164\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bbox_inches:\n\u001b[1;32m   2166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bbox_inches \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtight\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/automated_essay_scoring/.venv/lib/python3.11/site-packages/matplotlib/artist.py:95\u001b[0m, in \u001b[0;36m_finalize_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(draw)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdraw_wrapper\u001b[39m(artist, renderer, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 95\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m renderer\u001b[38;5;241m.\u001b[39m_rasterizing:\n\u001b[1;32m     97\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstop_rasterizing()\n",
      "File \u001b[0;32m~/automated_essay_scoring/.venv/lib/python3.11/site-packages/matplotlib/artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/automated_essay_scoring/.venv/lib/python3.11/site-packages/matplotlib/figure.py:3154\u001b[0m, in \u001b[0;36mFigure.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3151\u001b[0m         \u001b[38;5;66;03m# ValueError can occur when resizing a window.\u001b[39;00m\n\u001b[1;32m   3153\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch\u001b[38;5;241m.\u001b[39mdraw(renderer)\n\u001b[0;32m-> 3154\u001b[0m \u001b[43mmimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_draw_list_compositing_images\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuppressComposite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3157\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sfig \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubfigs:\n\u001b[1;32m   3158\u001b[0m     sfig\u001b[38;5;241m.\u001b[39mdraw(renderer)\n",
      "File \u001b[0;32m~/automated_essay_scoring/.venv/lib/python3.11/site-packages/matplotlib/image.py:132\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m not_composite \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_images:\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m artists:\n\u001b[0;32m--> 132\u001b[0m         \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m# Composite any adjacent images together\u001b[39;00m\n\u001b[1;32m    135\u001b[0m     image_group \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/automated_essay_scoring/.venv/lib/python3.11/site-packages/matplotlib/artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/automated_essay_scoring/.venv/lib/python3.11/site-packages/matplotlib/axes/_base.py:3070\u001b[0m, in \u001b[0;36m_AxesBase.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3067\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m artists_rasterized:\n\u001b[1;32m   3068\u001b[0m     _draw_rasterized(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure, artists_rasterized, renderer)\n\u001b[0;32m-> 3070\u001b[0m \u001b[43mmimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_draw_list_compositing_images\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3071\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuppressComposite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3073\u001b[0m renderer\u001b[38;5;241m.\u001b[39mclose_group(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maxes\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   3074\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/automated_essay_scoring/.venv/lib/python3.11/site-packages/matplotlib/image.py:132\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m not_composite \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_images:\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m artists:\n\u001b[0;32m--> 132\u001b[0m         \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m# Composite any adjacent images together\u001b[39;00m\n\u001b[1;32m    135\u001b[0m     image_group \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/automated_essay_scoring/.venv/lib/python3.11/site-packages/matplotlib/artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/automated_essay_scoring/.venv/lib/python3.11/site-packages/matplotlib/axis.py:1394\u001b[0m, in \u001b[0;36mAxis.draw\u001b[0;34m(self, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1391\u001b[0m     tick\u001b[38;5;241m.\u001b[39mdraw(renderer)\n\u001b[1;32m   1393\u001b[0m \u001b[38;5;66;03m# Shift label away from axes to avoid overlapping ticklabels.\u001b[39;00m\n\u001b[0;32m-> 1394\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_label_position\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1395\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel\u001b[38;5;241m.\u001b[39mdraw(renderer)\n\u001b[1;32m   1397\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_offset_text_position(tlb1, tlb2)\n",
      "File \u001b[0;32m~/automated_essay_scoring/.venv/lib/python3.11/site-packages/matplotlib/axis.py:2614\u001b[0m, in \u001b[0;36mYAxis._update_label_position\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   2612\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2613\u001b[0m     spine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes\u001b[38;5;241m.\u001b[39mspines[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m-> 2614\u001b[0m     spinebbox \u001b[38;5;241m=\u001b[39m \u001b[43mspine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_window_extent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2615\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m   2616\u001b[0m     \u001b[38;5;66;03m# use Axes if spine doesn't exist\u001b[39;00m\n\u001b[1;32m   2617\u001b[0m     spinebbox \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes\u001b[38;5;241m.\u001b[39mbbox\n",
      "File \u001b[0;32m~/automated_essay_scoring/.venv/lib/python3.11/site-packages/matplotlib/spines.py:158\u001b[0m, in \u001b[0;36mSpine.get_window_extent\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bb\n\u001b[1;32m    157\u001b[0m bboxes \u001b[38;5;241m=\u001b[39m [bb]\n\u001b[0;32m--> 158\u001b[0m drawn_ticks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_ticks\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m major_tick \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m({\u001b[38;5;241m*\u001b[39mdrawn_ticks} \u001b[38;5;241m&\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis\u001b[38;5;241m.\u001b[39mmajorTicks}), \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    161\u001b[0m minor_tick \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m({\u001b[38;5;241m*\u001b[39mdrawn_ticks} \u001b[38;5;241m&\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis\u001b[38;5;241m.\u001b[39mminorTicks}), \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/automated_essay_scoring/.venv/lib/python3.11/site-packages/matplotlib/axis.py:1279\u001b[0m, in \u001b[0;36mAxis._update_ticks\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1277\u001b[0m major_ticks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_major_ticks(\u001b[38;5;28mlen\u001b[39m(major_locs))\n\u001b[1;32m   1278\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tick, loc, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(major_ticks, major_locs, major_labels):\n\u001b[0;32m-> 1279\u001b[0m     \u001b[43mtick\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_position\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1280\u001b[0m     tick\u001b[38;5;241m.\u001b[39mlabel1\u001b[38;5;241m.\u001b[39mset_text(label)\n\u001b[1;32m   1281\u001b[0m     tick\u001b[38;5;241m.\u001b[39mlabel2\u001b[38;5;241m.\u001b[39mset_text(label)\n",
      "File \u001b[0;32m~/automated_essay_scoring/.venv/lib/python3.11/site-packages/matplotlib/axis.py:510\u001b[0m, in \u001b[0;36mYTick.update_position\u001b[0;34m(self, loc)\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_position\u001b[39m(\u001b[38;5;28mself\u001b[39m, loc):\n\u001b[1;32m    509\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Set the location of tick in data coords with scalar *loc*.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 510\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick1line\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_ydata\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    511\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtick2line\u001b[38;5;241m.\u001b[39mset_ydata((loc,))\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgridline\u001b[38;5;241m.\u001b[39mset_ydata((loc,))\n",
      "File \u001b[0;32m~/automated_essay_scoring/.venv/lib/python3.11/site-packages/matplotlib/lines.py:1308\u001b[0m, in \u001b[0;36mLine2D.set_ydata\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m   1302\u001b[0m     _api\u001b[38;5;241m.\u001b[39mwarn_deprecated(\n\u001b[1;32m   1303\u001b[0m         since\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3.7\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1304\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSetting data with a non sequence type \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1305\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis deprecated since \u001b[39m\u001b[38;5;132;01m%(since)s\u001b[39;00m\u001b[38;5;124m and will be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mremove \u001b[39m\u001b[38;5;132;01m%(removal)s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1307\u001b[0m     y \u001b[38;5;241m=\u001b[39m [y, ]\n\u001b[0;32m-> 1308\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_yorig \u001b[38;5;241m=\u001b[39m \u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1309\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_invalidy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1310\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/lib/python3.11/copy.py:74\u001b[0m, in \u001b[0;36mcopy\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Shallow copy operation on arbitrary Python objects.\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \n\u001b[1;32m     69\u001b[0m \u001b[38;5;124;03mSee the module's __doc__ string for more info.\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(x)\n\u001b[0;32m---> 74\u001b[0m copier \u001b[38;5;241m=\u001b[39m _copy_dispatch\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier:\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m copier(x)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 特徴量重要度を列にもつDataFrameを作成\n",
    "feature_importances = [\n",
    "    model.feature_importance(importance_type=\"gain\") for _, model in models\n",
    "]\n",
    "feature_importances_df = pd.DataFrame(feature_importances, columns=USE_COL)\n",
    "\n",
    "# 表示する順番を指定、特徴量重要度の平均が大きい順に並ぶよう計算\n",
    "order = feature_importances_df.mean().sort_values(ascending=False).index.tolist()\n",
    "\n",
    "# 表示\n",
    "# fold毎の特徴量重要度のばらつきを見るために、箱ひげ図を利用\n",
    "plt.figure(figsize=(10, 200))\n",
    "sns.boxplot(data=feature_importances_df, orient=\"h\", order=order)\n",
    "plt.savefig(f\"{MODEL_OUTPUT_PATH}/feature_importance.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CVの計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF CV Score by round: 0.7333994313287475\n"
     ]
    }
   ],
   "source": [
    "cv_score = cohen_kappa_score(\n",
    "    oof[\"score\"],\n",
    "    np.clip(oof[\"valid_pred\"], 1, 6).round(),\n",
    "    weights=\"quadratic\",\n",
    ")\n",
    "print(f\"OOF CV Score by round: {cv_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://qiita.com/kaggle_grandmaster-arai-san/items/d59b2fb7142ec7e270a5#optimizedrounder\n",
    "class OptimizedRounder:\n",
    "    def __init__(self):\n",
    "        self.coef_ = 0\n",
    "\n",
    "    def _kappa_loss(self, coef, X, y):\n",
    "        X_p = np.copy(X)\n",
    "        for i, pred in enumerate(X_p):\n",
    "            if pred < coef[0]:\n",
    "                X_p[i] = 1\n",
    "            elif pred >= coef[0] and pred < coef[1]:\n",
    "                X_p[i] = 2\n",
    "            elif pred >= coef[1] and pred < coef[2]:\n",
    "                X_p[i] = 3\n",
    "            elif pred >= coef[2] and pred < coef[3]:\n",
    "                X_p[i] = 4\n",
    "            elif pred >= coef[3] and pred < coef[4]:\n",
    "                X_p[i] = 5\n",
    "            else:\n",
    "                X_p[i] = 6\n",
    "\n",
    "        ll = cohen_kappa_score(y, X_p, weights=\"quadratic\")\n",
    "        return -ll\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        loss_partial = partial(self._kappa_loss, X=X, y=y)\n",
    "        initial_coef = [1.5, 2.5, 3.5, 4.5, 5.5]\n",
    "        self.coef_ = sp.optimize.minimize(\n",
    "            loss_partial, initial_coef, method=\"nelder-mead\"\n",
    "        )\n",
    "\n",
    "    def predict(self, X, coef):\n",
    "        X_p = np.copy(X)\n",
    "        for i, pred in enumerate(X_p):\n",
    "            if pred < coef[0]:\n",
    "                X_p[i] = 1\n",
    "            elif pred >= coef[0] and pred < coef[1]:\n",
    "                X_p[i] = 2\n",
    "            elif pred >= coef[1] and pred < coef[2]:\n",
    "                X_p[i] = 3\n",
    "            elif pred >= coef[2] and pred < coef[3]:\n",
    "                X_p[i] = 4\n",
    "            elif pred >= coef[3] and pred < coef[4]:\n",
    "                X_p[i] = 5\n",
    "            else:\n",
    "                X_p[i] = 6\n",
    "        return X_p\n",
    "\n",
    "    @property\n",
    "    def coefficients(self):\n",
    "        return self.coef_[\"x\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.9021168  2.52357984 3.26543861 3.96419971 5.86949709]\n",
      "OOF CV Score by NelderMead: 0.7762788117629078\n"
     ]
    }
   ],
   "source": [
    "optR = OptimizedRounder()\n",
    "optR.fit(oof[\"valid_pred\"], oof[\"score\"])\n",
    "print(optR.coefficients)\n",
    "\n",
    "optimized_valid_pred = optR.predict(oof[\"valid_pred\"], optR.coefficients)\n",
    "np.save(f\"{MODEL_OUTPUT_PATH}/opt_thr.npy\", optR.coefficients)\n",
    "\n",
    "cv_score = cohen_kappa_score(oof[\"score\"], optimized_valid_pred, weights=\"quadratic\")\n",
    "\n",
    "print(f\"OOF CV Score by NelderMead: {cv_score}\")\n",
    "\n",
    "# output_textを保存\n",
    "with open(f\"{MODEL_OUTPUT_PATH}/cv_score.txt\", \"w\") as f:\n",
    "    f.write(str(cv_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 混同行列の計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(\n",
    "    oof[\"score\"],\n",
    "    optimized_valid_pred,\n",
    "    labels=[x for x in range(1, 7)],\n",
    ")\n",
    "\n",
    "draw_cm = ConfusionMatrixDisplay(\n",
    "    confusion_matrix=cm, display_labels=[x for x in range(1, 7)]\n",
    ")\n",
    "\n",
    "draw_cm.plot()\n",
    "plt.savefig(f\"{MODEL_OUTPUT_PATH}/confusion_matrix.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggleへのアップロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if UPLOAD_DATA_TO_KAGGLE:\n",
    "    import os\n",
    "    import json\n",
    "\n",
    "    from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "    def dataset_create_new(dataset_name: str, upload_dir: str):\n",
    "        # if \"_\" in dataset_name:\n",
    "        #     raise ValueError(\"datasetの名称に_の使用は禁止です\")\n",
    "        dataset_metadata = {}\n",
    "        dataset_metadata[\"id\"] = f\"sinchir0/{dataset_name}\"\n",
    "        dataset_metadata[\"licenses\"] = [{\"name\": \"CC0-1.0\"}]\n",
    "        dataset_metadata[\"title\"] = dataset_name\n",
    "        with open(os.path.join(upload_dir, \"dataset-metadata.json\"), \"w\") as f:\n",
    "            json.dump(dataset_metadata, f, indent=4)\n",
    "        api = KaggleApi()\n",
    "        api.authenticate()\n",
    "        api.dataset_create_new(folder=upload_dir, convert_to_csv=False, dir_mode=\"tar\")\n",
    "\n",
    "    print(f\"Create Dataset name:{DATASET_NAME}, output_dir:{MODEL_OUTPUT_PATH}\")\n",
    "    dataset_create_new(dataset_name=DATASET_NAME, upload_dir=MODEL_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
